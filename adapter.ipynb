{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArchConfig:\n",
    "    encoder_block_nums:int\n",
    "    decoder_block_nums:int\n",
    "\n",
    "\n",
    "SUPPORTED_MODEL_ARCHS = {\n",
    "    # 'tiny',\n",
    "    # 'base',\n",
    "    # 'small',\n",
    "    'medium': ModelArchConfig(24, 24),\n",
    "    'large': ModelArchConfig(32, 32),\n",
    "}\n",
    "\n",
    "WHISPER_HF_ENCODER_BLOCK_MAPPING = {\n",
    "    'encoder.blocks.{i}.attn.query.weight' :'model.encoder.layers.{i}.self_attn.q_proj.weight',\n",
    "    'encoder.blocks.{i}.attn.query.bias'   :'model.encoder.layers.{i}.self_attn.q_proj.bias',\n",
    "    'encoder.blocks.{i}.attn.key.weight'   :'model.encoder.layers.{i}.self_attn.k_proj.weight',\n",
    "    'encoder.blocks.{i}.attn.value.weight' :'model.encoder.layers.{i}.self_attn.v_proj.weight',\n",
    "    'encoder.blocks.{i}.attn.value.bias'   :'model.encoder.layers.{i}.self_attn.v_proj.bias',\n",
    "    'encoder.blocks.{i}.attn.out.weight'   :'model.encoder.layers.{i}.self_attn.out_proj.weight',\n",
    "    'encoder.blocks.{i}.attn.out.bias'     :'model.encoder.layers.{i}.self_attn.out_proj.bias',\n",
    "    'encoder.blocks.{i}.attn_ln.weight'    :'model.encoder.layers.{i}.self_attn_layer_norm.weight',\n",
    "    'encoder.blocks.{i}.attn_ln.bias'      :'model.encoder.layers.{i}.self_attn_layer_norm.bias',\n",
    "    'encoder.blocks.{i}.mlp.0.weight'      :'model.encoder.layers.{i}.fc1.weight',\n",
    "    'encoder.blocks.{i}.mlp.0.bias'        :'model.encoder.layers.{i}.fc1.bias',\n",
    "    'encoder.blocks.{i}.mlp.2.weight'      :'model.encoder.layers.{i}.fc2.weight',\n",
    "    'encoder.blocks.{i}.mlp.2.bias'        :'model.encoder.layers.{i}.fc2.bias',\n",
    "    'encoder.blocks.{i}.mlp_ln.weight'     :'model.encoder.layers.{i}.final_layer_norm.weight',\n",
    "    'encoder.blocks.{i}.mlp_ln.bias'       :'model.encoder.layers.{i}.final_layer_norm.bias',\n",
    "}\n",
    "\n",
    "\n",
    "WHISPER_HF_DECODER_BLOCK_MAPPING = {\n",
    "    'decoder.blocks.{i}.attn.query.weight'       :'model.decoder.layers.{i}.self_attn.q_proj.weight',\n",
    "    'decoder.blocks.{i}.attn.query.bias'         :'model.decoder.layers.{i}.self_attn.q_proj.bias',\n",
    "    'decoder.blocks.{i}.attn.key.weight'         :'model.decoder.layers.{i}.self_attn.k_proj.weight',\n",
    "    'decoder.blocks.{i}.attn.value.weight'       :'model.decoder.layers.{i}.self_attn.v_proj.weight',\n",
    "    'decoder.blocks.{i}.attn.value.bias'         :'model.decoder.layers.{i}.self_attn.v_proj.bias',\n",
    "    'decoder.blocks.{i}.attn.out.weight'         :'model.decoder.layers.{i}.self_attn.out_proj.weight',\n",
    "    'decoder.blocks.{i}.attn.out.bias'           :'model.decoder.layers.{i}.self_attn.out_proj.bias',\n",
    "    'decoder.blocks.{i}.attn_ln.weight'          :'model.decoder.layers.{i}.self_attn_layer_norm.weight',\n",
    "    'decoder.blocks.{i}.attn_ln.bias'            :'model.decoder.layers.{i}.self_attn_layer_norm.bias',\n",
    "    'decoder.blocks.{i}.cross_attn.query.weight' :'model.decoder.layers.{i}.encoder_attn.q_proj.weight',\n",
    "    'decoder.blocks.{i}.cross_attn.query.bias'   :'model.decoder.layers.{i}.encoder_attn.q_proj.bias',\n",
    "    'decoder.blocks.{i}.cross_attn.key.weight'   :'model.decoder.layers.{i}.encoder_attn.k_proj.weight',\n",
    "    'decoder.blocks.{i}.cross_attn.value.weight' :'model.decoder.layers.{i}.encoder_attn.v_proj.weight',\n",
    "    'decoder.blocks.{i}.cross_attn.value.bias'   :'model.decoder.layers.{i}.encoder_attn.v_proj.bias',\n",
    "    'decoder.blocks.{i}.cross_attn.out.weight'   :'model.decoder.layers.{i}.encoder_attn.out_proj.weight',\n",
    "    'decoder.blocks.{i}.cross_attn.out.bias'     :'model.decoder.layers.{i}.encoder_attn.out_proj.bias',\n",
    "    'decoder.blocks.{i}.cross_attn_ln.weight'    :'model.decoder.layers.{i}.encoder_attn_layer_norm.weight',\n",
    "    'decoder.blocks.{i}.cross_attn_ln.bias'      :'model.decoder.layers.{i}.encoder_attn_layer_norm.bias',\n",
    "    'decoder.blocks.{i}.mlp.0.weight'            :'model.decoder.layers.{i}.fc1.weight',\n",
    "    'decoder.blocks.{i}.mlp.0.bias'              :'model.decoder.layers.{i}.fc1.bias',\n",
    "    'decoder.blocks.{i}.mlp.2.weight'            :'model.decoder.layers.{i}.fc2.weight',\n",
    "    'decoder.blocks.{i}.mlp.2.bias'              :'model.decoder.layers.{i}.fc2.bias',\n",
    "    'decoder.blocks.{i}.mlp_ln.weight'           :'model.decoder.layers.{i}.final_layer_norm.weight',\n",
    "    'decoder.blocks.{i}.mlp_ln.bias'             :'model.decoder.layers.{i}.final_layer_norm.bias',\n",
    "}\n",
    "\n",
    "WHISPER_HF_MAPPING = {\n",
    "\n",
    "    # encoder\n",
    "    'encoder.conv1.weight' :'model.encoder.conv1.weight',\n",
    "    'encoder.conv1.bias'   :'model.encoder.conv1.bias',\n",
    "    'encoder.conv2.weight' :'model.encoder.conv2.weight',\n",
    "    'encoder.conv2.bias'   :'model.encoder.conv2.bias',\n",
    "    # ???:'model.encoder.embed_positions.weight',\n",
    "    **{block_name.format(i=i):hf_block_name.format(i=i)\n",
    "     for i in range(24)\n",
    "     for block_name, hf_block_name in WHISPER_HF_ENCODER_BLOCK_MAPPING.items()},\n",
    "    'encoder.ln_post.weight':'model.encoder.layer_norm.weight',\n",
    "    'encoder.ln_post.bias'  :'model.encoder.layer_norm.bias',\n",
    "\n",
    "    # decoder\n",
    "    'decoder.positional_embedding':'model.decoder.embed_tokens.weight',\n",
    "    'decoder.token_embedding.weight':'model.decoder.embed_positions.weight',\n",
    "    **{block_name.format(i=i):hf_block_name.format(i=i)\n",
    "     for i in range(24)\n",
    "     for block_name, hf_block_name in WHISPER_HF_DECODER_BLOCK_MAPPING.items()},\n",
    "    'decoder.ln.weight'     : 'model.decoder.layer_norm.weight',\n",
    "    'decoder.ln.bias'       : 'model.decoder.layer_norm.bias',\n",
    "}\n",
    "\n",
    "\n",
    "class WhisperHFArchMapping:\n",
    "    def __init__(self, model_arch):\n",
    "        self.model_arch = model_arch\n",
    "        self.encoder_block_nums = SUPPORTED_MODEL_ARCHS[model_arch].encoder_block_nums\n",
    "        self.decoder_block_nums = SUPPORTED_MODEL_ARCHS[model_arch].decoder_block_nums\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder.conv1.weight': 'model.encoder.conv1.weight',\n",
       " 'encoder.conv1.bias': 'model.encoder.conv1.bias',\n",
       " 'encoder.conv2.weight': 'model.encoder.conv2.weight',\n",
       " 'encoder.conv2.bias': 'model.encoder.conv2.bias',\n",
       " 'encoder.blocks.0.attn.query.weight': 'model.encoder.layers.0.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.0.attn.query.bias': 'model.encoder.layers.0.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.0.attn.key.weight': 'model.encoder.layers.0.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.0.attn.value.weight': 'model.encoder.layers.0.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.0.attn.value.bias': 'model.encoder.layers.0.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.0.attn.out.weight': 'model.encoder.layers.0.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.0.attn.out.bias': 'model.encoder.layers.0.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.0.attn_ln.weight': 'model.encoder.layers.0.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.0.attn_ln.bias': 'model.encoder.layers.0.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.0.mlp.2.weight': 'model.encoder.layers.0.fc2.weight',\n",
       " 'encoder.blocks.0.mlp.2.bias': 'model.encoder.layers.0.fc2.bias',\n",
       " 'encoder.blocks.0.mlp_ln.weight': 'model.encoder.layers.0.final_layer_norm.weight',\n",
       " 'encoder.blocks.0.mlp_ln.bias': 'model.encoder.layers.0.final_layer_norm.bias',\n",
       " 'encoder.blocks.1.attn.query.weight': 'model.encoder.layers.1.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.1.attn.query.bias': 'model.encoder.layers.1.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.1.attn.key.weight': 'model.encoder.layers.1.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.1.attn.value.weight': 'model.encoder.layers.1.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.1.attn.value.bias': 'model.encoder.layers.1.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.1.attn.out.weight': 'model.encoder.layers.1.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.1.attn.out.bias': 'model.encoder.layers.1.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.1.attn_ln.weight': 'model.encoder.layers.1.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.1.attn_ln.bias': 'model.encoder.layers.1.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.1.mlp.2.weight': 'model.encoder.layers.1.fc2.weight',\n",
       " 'encoder.blocks.1.mlp.2.bias': 'model.encoder.layers.1.fc2.bias',\n",
       " 'encoder.blocks.1.mlp_ln.weight': 'model.encoder.layers.1.final_layer_norm.weight',\n",
       " 'encoder.blocks.1.mlp_ln.bias': 'model.encoder.layers.1.final_layer_norm.bias',\n",
       " 'encoder.blocks.2.attn.query.weight': 'model.encoder.layers.2.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.2.attn.query.bias': 'model.encoder.layers.2.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.2.attn.key.weight': 'model.encoder.layers.2.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.2.attn.value.weight': 'model.encoder.layers.2.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.2.attn.value.bias': 'model.encoder.layers.2.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.2.attn.out.weight': 'model.encoder.layers.2.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.2.attn.out.bias': 'model.encoder.layers.2.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.2.attn_ln.weight': 'model.encoder.layers.2.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.2.attn_ln.bias': 'model.encoder.layers.2.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.2.mlp.2.weight': 'model.encoder.layers.2.fc2.weight',\n",
       " 'encoder.blocks.2.mlp.2.bias': 'model.encoder.layers.2.fc2.bias',\n",
       " 'encoder.blocks.2.mlp_ln.weight': 'model.encoder.layers.2.final_layer_norm.weight',\n",
       " 'encoder.blocks.2.mlp_ln.bias': 'model.encoder.layers.2.final_layer_norm.bias',\n",
       " 'encoder.blocks.3.attn.query.weight': 'model.encoder.layers.3.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.3.attn.query.bias': 'model.encoder.layers.3.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.3.attn.key.weight': 'model.encoder.layers.3.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.3.attn.value.weight': 'model.encoder.layers.3.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.3.attn.value.bias': 'model.encoder.layers.3.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.3.attn.out.weight': 'model.encoder.layers.3.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.3.attn.out.bias': 'model.encoder.layers.3.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.3.attn_ln.weight': 'model.encoder.layers.3.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.3.attn_ln.bias': 'model.encoder.layers.3.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.3.mlp.2.weight': 'model.encoder.layers.3.fc2.weight',\n",
       " 'encoder.blocks.3.mlp.2.bias': 'model.encoder.layers.3.fc2.bias',\n",
       " 'encoder.blocks.3.mlp_ln.weight': 'model.encoder.layers.3.final_layer_norm.weight',\n",
       " 'encoder.blocks.3.mlp_ln.bias': 'model.encoder.layers.3.final_layer_norm.bias',\n",
       " 'encoder.blocks.4.attn.query.weight': 'model.encoder.layers.4.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.4.attn.query.bias': 'model.encoder.layers.4.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.4.attn.key.weight': 'model.encoder.layers.4.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.4.attn.value.weight': 'model.encoder.layers.4.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.4.attn.value.bias': 'model.encoder.layers.4.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.4.attn.out.weight': 'model.encoder.layers.4.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.4.attn.out.bias': 'model.encoder.layers.4.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.4.attn_ln.weight': 'model.encoder.layers.4.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.4.attn_ln.bias': 'model.encoder.layers.4.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.4.mlp.2.weight': 'model.encoder.layers.4.fc2.weight',\n",
       " 'encoder.blocks.4.mlp.2.bias': 'model.encoder.layers.4.fc2.bias',\n",
       " 'encoder.blocks.4.mlp_ln.weight': 'model.encoder.layers.4.final_layer_norm.weight',\n",
       " 'encoder.blocks.4.mlp_ln.bias': 'model.encoder.layers.4.final_layer_norm.bias',\n",
       " 'encoder.blocks.5.attn.query.weight': 'model.encoder.layers.5.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.5.attn.query.bias': 'model.encoder.layers.5.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.5.attn.key.weight': 'model.encoder.layers.5.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.5.attn.value.weight': 'model.encoder.layers.5.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.5.attn.value.bias': 'model.encoder.layers.5.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.5.attn.out.weight': 'model.encoder.layers.5.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.5.attn.out.bias': 'model.encoder.layers.5.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.5.attn_ln.weight': 'model.encoder.layers.5.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.5.attn_ln.bias': 'model.encoder.layers.5.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.5.mlp.2.weight': 'model.encoder.layers.5.fc2.weight',\n",
       " 'encoder.blocks.5.mlp.2.bias': 'model.encoder.layers.5.fc2.bias',\n",
       " 'encoder.blocks.5.mlp_ln.weight': 'model.encoder.layers.5.final_layer_norm.weight',\n",
       " 'encoder.blocks.5.mlp_ln.bias': 'model.encoder.layers.5.final_layer_norm.bias',\n",
       " 'encoder.blocks.6.attn.query.weight': 'model.encoder.layers.6.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.6.attn.query.bias': 'model.encoder.layers.6.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.6.attn.key.weight': 'model.encoder.layers.6.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.6.attn.value.weight': 'model.encoder.layers.6.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.6.attn.value.bias': 'model.encoder.layers.6.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.6.attn.out.weight': 'model.encoder.layers.6.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.6.attn.out.bias': 'model.encoder.layers.6.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.6.attn_ln.weight': 'model.encoder.layers.6.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.6.attn_ln.bias': 'model.encoder.layers.6.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.6.mlp.2.weight': 'model.encoder.layers.6.fc2.weight',\n",
       " 'encoder.blocks.6.mlp.2.bias': 'model.encoder.layers.6.fc2.bias',\n",
       " 'encoder.blocks.6.mlp_ln.weight': 'model.encoder.layers.6.final_layer_norm.weight',\n",
       " 'encoder.blocks.6.mlp_ln.bias': 'model.encoder.layers.6.final_layer_norm.bias',\n",
       " 'encoder.blocks.7.attn.query.weight': 'model.encoder.layers.7.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.7.attn.query.bias': 'model.encoder.layers.7.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.7.attn.key.weight': 'model.encoder.layers.7.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.7.attn.value.weight': 'model.encoder.layers.7.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.7.attn.value.bias': 'model.encoder.layers.7.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.7.attn.out.weight': 'model.encoder.layers.7.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.7.attn.out.bias': 'model.encoder.layers.7.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.7.attn_ln.weight': 'model.encoder.layers.7.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.7.attn_ln.bias': 'model.encoder.layers.7.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.7.mlp.2.weight': 'model.encoder.layers.7.fc2.weight',\n",
       " 'encoder.blocks.7.mlp.2.bias': 'model.encoder.layers.7.fc2.bias',\n",
       " 'encoder.blocks.7.mlp_ln.weight': 'model.encoder.layers.7.final_layer_norm.weight',\n",
       " 'encoder.blocks.7.mlp_ln.bias': 'model.encoder.layers.7.final_layer_norm.bias',\n",
       " 'encoder.blocks.8.attn.query.weight': 'model.encoder.layers.8.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.8.attn.query.bias': 'model.encoder.layers.8.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.8.attn.key.weight': 'model.encoder.layers.8.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.8.attn.value.weight': 'model.encoder.layers.8.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.8.attn.value.bias': 'model.encoder.layers.8.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.8.attn.out.weight': 'model.encoder.layers.8.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.8.attn.out.bias': 'model.encoder.layers.8.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.8.attn_ln.weight': 'model.encoder.layers.8.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.8.attn_ln.bias': 'model.encoder.layers.8.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.8.mlp.2.weight': 'model.encoder.layers.8.fc2.weight',\n",
       " 'encoder.blocks.8.mlp.2.bias': 'model.encoder.layers.8.fc2.bias',\n",
       " 'encoder.blocks.8.mlp_ln.weight': 'model.encoder.layers.8.final_layer_norm.weight',\n",
       " 'encoder.blocks.8.mlp_ln.bias': 'model.encoder.layers.8.final_layer_norm.bias',\n",
       " 'encoder.blocks.9.attn.query.weight': 'model.encoder.layers.9.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.9.attn.query.bias': 'model.encoder.layers.9.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.9.attn.key.weight': 'model.encoder.layers.9.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.9.attn.value.weight': 'model.encoder.layers.9.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.9.attn.value.bias': 'model.encoder.layers.9.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.9.attn.out.weight': 'model.encoder.layers.9.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.9.attn.out.bias': 'model.encoder.layers.9.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.9.attn_ln.weight': 'model.encoder.layers.9.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.9.attn_ln.bias': 'model.encoder.layers.9.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.9.mlp.2.weight': 'model.encoder.layers.9.fc2.weight',\n",
       " 'encoder.blocks.9.mlp.2.bias': 'model.encoder.layers.9.fc2.bias',\n",
       " 'encoder.blocks.9.mlp_ln.weight': 'model.encoder.layers.9.final_layer_norm.weight',\n",
       " 'encoder.blocks.9.mlp_ln.bias': 'model.encoder.layers.9.final_layer_norm.bias',\n",
       " 'encoder.blocks.10.attn.query.weight': 'model.encoder.layers.10.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.10.attn.query.bias': 'model.encoder.layers.10.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.10.attn.key.weight': 'model.encoder.layers.10.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.10.attn.value.weight': 'model.encoder.layers.10.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.10.attn.value.bias': 'model.encoder.layers.10.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.10.attn.out.weight': 'model.encoder.layers.10.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.10.attn.out.bias': 'model.encoder.layers.10.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.10.attn_ln.weight': 'model.encoder.layers.10.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.10.attn_ln.bias': 'model.encoder.layers.10.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.10.mlp.2.weight': 'model.encoder.layers.10.fc2.weight',\n",
       " 'encoder.blocks.10.mlp.2.bias': 'model.encoder.layers.10.fc2.bias',\n",
       " 'encoder.blocks.10.mlp_ln.weight': 'model.encoder.layers.10.final_layer_norm.weight',\n",
       " 'encoder.blocks.10.mlp_ln.bias': 'model.encoder.layers.10.final_layer_norm.bias',\n",
       " 'encoder.blocks.11.attn.query.weight': 'model.encoder.layers.11.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.11.attn.query.bias': 'model.encoder.layers.11.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.11.attn.key.weight': 'model.encoder.layers.11.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.11.attn.value.weight': 'model.encoder.layers.11.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.11.attn.value.bias': 'model.encoder.layers.11.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.11.attn.out.weight': 'model.encoder.layers.11.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.11.attn.out.bias': 'model.encoder.layers.11.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.11.attn_ln.weight': 'model.encoder.layers.11.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.11.attn_ln.bias': 'model.encoder.layers.11.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.11.mlp.2.weight': 'model.encoder.layers.11.fc2.weight',\n",
       " 'encoder.blocks.11.mlp.2.bias': 'model.encoder.layers.11.fc2.bias',\n",
       " 'encoder.blocks.11.mlp_ln.weight': 'model.encoder.layers.11.final_layer_norm.weight',\n",
       " 'encoder.blocks.11.mlp_ln.bias': 'model.encoder.layers.11.final_layer_norm.bias',\n",
       " 'encoder.blocks.12.attn.query.weight': 'model.encoder.layers.12.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.12.attn.query.bias': 'model.encoder.layers.12.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.12.attn.key.weight': 'model.encoder.layers.12.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.12.attn.value.weight': 'model.encoder.layers.12.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.12.attn.value.bias': 'model.encoder.layers.12.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.12.attn.out.weight': 'model.encoder.layers.12.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.12.attn.out.bias': 'model.encoder.layers.12.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.12.attn_ln.weight': 'model.encoder.layers.12.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.12.attn_ln.bias': 'model.encoder.layers.12.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.12.mlp.2.weight': 'model.encoder.layers.12.fc2.weight',\n",
       " 'encoder.blocks.12.mlp.2.bias': 'model.encoder.layers.12.fc2.bias',\n",
       " 'encoder.blocks.12.mlp_ln.weight': 'model.encoder.layers.12.final_layer_norm.weight',\n",
       " 'encoder.blocks.12.mlp_ln.bias': 'model.encoder.layers.12.final_layer_norm.bias',\n",
       " 'encoder.blocks.13.attn.query.weight': 'model.encoder.layers.13.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.13.attn.query.bias': 'model.encoder.layers.13.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.13.attn.key.weight': 'model.encoder.layers.13.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.13.attn.value.weight': 'model.encoder.layers.13.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.13.attn.value.bias': 'model.encoder.layers.13.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.13.attn.out.weight': 'model.encoder.layers.13.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.13.attn.out.bias': 'model.encoder.layers.13.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.13.attn_ln.weight': 'model.encoder.layers.13.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.13.attn_ln.bias': 'model.encoder.layers.13.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.13.mlp.2.weight': 'model.encoder.layers.13.fc2.weight',\n",
       " 'encoder.blocks.13.mlp.2.bias': 'model.encoder.layers.13.fc2.bias',\n",
       " 'encoder.blocks.13.mlp_ln.weight': 'model.encoder.layers.13.final_layer_norm.weight',\n",
       " 'encoder.blocks.13.mlp_ln.bias': 'model.encoder.layers.13.final_layer_norm.bias',\n",
       " 'encoder.blocks.14.attn.query.weight': 'model.encoder.layers.14.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.14.attn.query.bias': 'model.encoder.layers.14.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.14.attn.key.weight': 'model.encoder.layers.14.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.14.attn.value.weight': 'model.encoder.layers.14.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.14.attn.value.bias': 'model.encoder.layers.14.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.14.attn.out.weight': 'model.encoder.layers.14.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.14.attn.out.bias': 'model.encoder.layers.14.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.14.attn_ln.weight': 'model.encoder.layers.14.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.14.attn_ln.bias': 'model.encoder.layers.14.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.14.mlp.2.weight': 'model.encoder.layers.14.fc2.weight',\n",
       " 'encoder.blocks.14.mlp.2.bias': 'model.encoder.layers.14.fc2.bias',\n",
       " 'encoder.blocks.14.mlp_ln.weight': 'model.encoder.layers.14.final_layer_norm.weight',\n",
       " 'encoder.blocks.14.mlp_ln.bias': 'model.encoder.layers.14.final_layer_norm.bias',\n",
       " 'encoder.blocks.15.attn.query.weight': 'model.encoder.layers.15.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.15.attn.query.bias': 'model.encoder.layers.15.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.15.attn.key.weight': 'model.encoder.layers.15.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.15.attn.value.weight': 'model.encoder.layers.15.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.15.attn.value.bias': 'model.encoder.layers.15.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.15.attn.out.weight': 'model.encoder.layers.15.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.15.attn.out.bias': 'model.encoder.layers.15.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.15.attn_ln.weight': 'model.encoder.layers.15.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.15.attn_ln.bias': 'model.encoder.layers.15.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.15.mlp.2.weight': 'model.encoder.layers.15.fc2.weight',\n",
       " 'encoder.blocks.15.mlp.2.bias': 'model.encoder.layers.15.fc2.bias',\n",
       " 'encoder.blocks.15.mlp_ln.weight': 'model.encoder.layers.15.final_layer_norm.weight',\n",
       " 'encoder.blocks.15.mlp_ln.bias': 'model.encoder.layers.15.final_layer_norm.bias',\n",
       " 'encoder.blocks.16.attn.query.weight': 'model.encoder.layers.16.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.16.attn.query.bias': 'model.encoder.layers.16.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.16.attn.key.weight': 'model.encoder.layers.16.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.16.attn.value.weight': 'model.encoder.layers.16.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.16.attn.value.bias': 'model.encoder.layers.16.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.16.attn.out.weight': 'model.encoder.layers.16.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.16.attn.out.bias': 'model.encoder.layers.16.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.16.attn_ln.weight': 'model.encoder.layers.16.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.16.attn_ln.bias': 'model.encoder.layers.16.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.16.mlp.2.weight': 'model.encoder.layers.16.fc2.weight',\n",
       " 'encoder.blocks.16.mlp.2.bias': 'model.encoder.layers.16.fc2.bias',\n",
       " 'encoder.blocks.16.mlp_ln.weight': 'model.encoder.layers.16.final_layer_norm.weight',\n",
       " 'encoder.blocks.16.mlp_ln.bias': 'model.encoder.layers.16.final_layer_norm.bias',\n",
       " 'encoder.blocks.17.attn.query.weight': 'model.encoder.layers.17.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.17.attn.query.bias': 'model.encoder.layers.17.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.17.attn.key.weight': 'model.encoder.layers.17.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.17.attn.value.weight': 'model.encoder.layers.17.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.17.attn.value.bias': 'model.encoder.layers.17.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.17.attn.out.weight': 'model.encoder.layers.17.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.17.attn.out.bias': 'model.encoder.layers.17.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.17.attn_ln.weight': 'model.encoder.layers.17.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.17.attn_ln.bias': 'model.encoder.layers.17.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.17.mlp.2.weight': 'model.encoder.layers.17.fc2.weight',\n",
       " 'encoder.blocks.17.mlp.2.bias': 'model.encoder.layers.17.fc2.bias',\n",
       " 'encoder.blocks.17.mlp_ln.weight': 'model.encoder.layers.17.final_layer_norm.weight',\n",
       " 'encoder.blocks.17.mlp_ln.bias': 'model.encoder.layers.17.final_layer_norm.bias',\n",
       " 'encoder.blocks.18.attn.query.weight': 'model.encoder.layers.18.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.18.attn.query.bias': 'model.encoder.layers.18.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.18.attn.key.weight': 'model.encoder.layers.18.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.18.attn.value.weight': 'model.encoder.layers.18.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.18.attn.value.bias': 'model.encoder.layers.18.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.18.attn.out.weight': 'model.encoder.layers.18.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.18.attn.out.bias': 'model.encoder.layers.18.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.18.attn_ln.weight': 'model.encoder.layers.18.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.18.attn_ln.bias': 'model.encoder.layers.18.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.18.mlp.2.weight': 'model.encoder.layers.18.fc2.weight',\n",
       " 'encoder.blocks.18.mlp.2.bias': 'model.encoder.layers.18.fc2.bias',\n",
       " 'encoder.blocks.18.mlp_ln.weight': 'model.encoder.layers.18.final_layer_norm.weight',\n",
       " 'encoder.blocks.18.mlp_ln.bias': 'model.encoder.layers.18.final_layer_norm.bias',\n",
       " 'encoder.blocks.19.attn.query.weight': 'model.encoder.layers.19.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.19.attn.query.bias': 'model.encoder.layers.19.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.19.attn.key.weight': 'model.encoder.layers.19.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.19.attn.value.weight': 'model.encoder.layers.19.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.19.attn.value.bias': 'model.encoder.layers.19.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.19.attn.out.weight': 'model.encoder.layers.19.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.19.attn.out.bias': 'model.encoder.layers.19.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.19.attn_ln.weight': 'model.encoder.layers.19.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.19.attn_ln.bias': 'model.encoder.layers.19.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.19.mlp.2.weight': 'model.encoder.layers.19.fc2.weight',\n",
       " 'encoder.blocks.19.mlp.2.bias': 'model.encoder.layers.19.fc2.bias',\n",
       " 'encoder.blocks.19.mlp_ln.weight': 'model.encoder.layers.19.final_layer_norm.weight',\n",
       " 'encoder.blocks.19.mlp_ln.bias': 'model.encoder.layers.19.final_layer_norm.bias',\n",
       " 'encoder.blocks.20.attn.query.weight': 'model.encoder.layers.20.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.20.attn.query.bias': 'model.encoder.layers.20.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.20.attn.key.weight': 'model.encoder.layers.20.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.20.attn.value.weight': 'model.encoder.layers.20.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.20.attn.value.bias': 'model.encoder.layers.20.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.20.attn.out.weight': 'model.encoder.layers.20.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.20.attn.out.bias': 'model.encoder.layers.20.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.20.attn_ln.weight': 'model.encoder.layers.20.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.20.attn_ln.bias': 'model.encoder.layers.20.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.20.mlp.2.weight': 'model.encoder.layers.20.fc2.weight',\n",
       " 'encoder.blocks.20.mlp.2.bias': 'model.encoder.layers.20.fc2.bias',\n",
       " 'encoder.blocks.20.mlp_ln.weight': 'model.encoder.layers.20.final_layer_norm.weight',\n",
       " 'encoder.blocks.20.mlp_ln.bias': 'model.encoder.layers.20.final_layer_norm.bias',\n",
       " 'encoder.blocks.21.attn.query.weight': 'model.encoder.layers.21.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.21.attn.query.bias': 'model.encoder.layers.21.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.21.attn.key.weight': 'model.encoder.layers.21.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.21.attn.value.weight': 'model.encoder.layers.21.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.21.attn.value.bias': 'model.encoder.layers.21.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.21.attn.out.weight': 'model.encoder.layers.21.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.21.attn.out.bias': 'model.encoder.layers.21.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.21.attn_ln.weight': 'model.encoder.layers.21.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.21.attn_ln.bias': 'model.encoder.layers.21.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.21.mlp.2.weight': 'model.encoder.layers.21.fc2.weight',\n",
       " 'encoder.blocks.21.mlp.2.bias': 'model.encoder.layers.21.fc2.bias',\n",
       " 'encoder.blocks.21.mlp_ln.weight': 'model.encoder.layers.21.final_layer_norm.weight',\n",
       " 'encoder.blocks.21.mlp_ln.bias': 'model.encoder.layers.21.final_layer_norm.bias',\n",
       " 'encoder.blocks.22.attn.query.weight': 'model.encoder.layers.22.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.22.attn.query.bias': 'model.encoder.layers.22.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.22.attn.key.weight': 'model.encoder.layers.22.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.22.attn.value.weight': 'model.encoder.layers.22.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.22.attn.value.bias': 'model.encoder.layers.22.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.22.attn.out.weight': 'model.encoder.layers.22.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.22.attn.out.bias': 'model.encoder.layers.22.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.22.attn_ln.weight': 'model.encoder.layers.22.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.22.attn_ln.bias': 'model.encoder.layers.22.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.22.mlp.2.weight': 'model.encoder.layers.22.fc2.weight',\n",
       " 'encoder.blocks.22.mlp.2.bias': 'model.encoder.layers.22.fc2.bias',\n",
       " 'encoder.blocks.22.mlp_ln.weight': 'model.encoder.layers.22.final_layer_norm.weight',\n",
       " 'encoder.blocks.22.mlp_ln.bias': 'model.encoder.layers.22.final_layer_norm.bias',\n",
       " 'encoder.blocks.23.attn.query.weight': 'model.encoder.layers.23.self_attn.q_proj.weight',\n",
       " 'encoder.blocks.23.attn.query.bias': 'model.encoder.layers.23.self_attn.q_proj.bias',\n",
       " 'encoder.blocks.23.attn.key.weight': 'model.encoder.layers.23.self_attn.k_proj.weight',\n",
       " 'encoder.blocks.23.attn.value.weight': 'model.encoder.layers.23.self_attn.v_proj.weight',\n",
       " 'encoder.blocks.23.attn.value.bias': 'model.encoder.layers.23.self_attn.v_proj.bias',\n",
       " 'encoder.blocks.23.attn.out.weight': 'model.encoder.layers.23.self_attn.out_proj.weight',\n",
       " 'encoder.blocks.23.attn.out.bias': 'model.encoder.layers.23.self_attn.out_proj.bias',\n",
       " 'encoder.blocks.23.attn_ln.weight': 'model.encoder.layers.23.self_attn_layer_norm.weight',\n",
       " 'encoder.blocks.23.attn_ln.bias': 'model.encoder.layers.23.self_attn_layer_norm.bias',\n",
       " 'encoder.blocks.23.mlp.2.weight': 'model.encoder.layers.23.fc2.weight',\n",
       " 'encoder.blocks.23.mlp.2.bias': 'model.encoder.layers.23.fc2.bias',\n",
       " 'encoder.blocks.23.mlp_ln.weight': 'model.encoder.layers.23.final_layer_norm.weight',\n",
       " 'encoder.blocks.23.mlp_ln.bias': 'model.encoder.layers.23.final_layer_norm.bias',\n",
       " 'encoder.ln_post.weight': 'model.encoder.layer_norm.weight',\n",
       " 'encoder.ln_post.bias': 'model.encoder.layer_norm.bias',\n",
       " 'decoder.positional_embedding': 'model.decoder.embed_tokens.weight',\n",
       " 'decoder.token_embedding.weight': 'model.decoder.embed_positions.weight',\n",
       " 'decoder.blocks.0.attn.query.weight': 'model.decoder.layers.0.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.0.attn.query.bias': 'model.decoder.layers.0.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.0.attn.key.weight': 'model.decoder.layers.0.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.0.attn.value.weight': 'model.decoder.layers.0.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.0.attn.value.bias': 'model.decoder.layers.0.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.0.attn.out.weight': 'model.decoder.layers.0.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.0.attn.out.bias': 'model.decoder.layers.0.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.0.attn_ln.weight': 'model.decoder.layers.0.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.0.attn_ln.bias': 'model.decoder.layers.0.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.0.cross_attn.query.weight': 'model.decoder.layers.0.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.0.cross_attn.query.bias': 'model.decoder.layers.0.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.0.cross_attn.key.weight': 'model.decoder.layers.0.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.0.cross_attn.value.weight': 'model.decoder.layers.0.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.0.cross_attn.value.bias': 'model.decoder.layers.0.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.0.cross_attn.out.weight': 'model.decoder.layers.0.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.0.cross_attn.out.bias': 'model.decoder.layers.0.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.0.cross_attn_ln.weight': 'model.decoder.layers.0.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.0.cross_attn_ln.bias': 'model.decoder.layers.0.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.0.mlp.0.weight': 'model.decoder.layers.0.fc1.weight',\n",
       " 'decoder.blocks.0.mlp.0.bias': 'model.decoder.layers.0.fc1.bias',\n",
       " 'decoder.blocks.0.mlp.2.weight': 'model.decoder.layers.0.fc2.weight',\n",
       " 'decoder.blocks.0.mlp.2.bias': 'model.decoder.layers.0.fc2.bias',\n",
       " 'decoder.blocks.0.mlp_ln.weight': 'model.decoder.layers.0.final_layer_norm.weight',\n",
       " 'decoder.blocks.0.mlp_ln.bias': 'model.decoder.layers.0.final_layer_norm.bias',\n",
       " 'decoder.blocks.1.attn.query.weight': 'model.decoder.layers.1.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.1.attn.query.bias': 'model.decoder.layers.1.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.1.attn.key.weight': 'model.decoder.layers.1.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.1.attn.value.weight': 'model.decoder.layers.1.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.1.attn.value.bias': 'model.decoder.layers.1.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.1.attn.out.weight': 'model.decoder.layers.1.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.1.attn.out.bias': 'model.decoder.layers.1.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.1.attn_ln.weight': 'model.decoder.layers.1.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.1.attn_ln.bias': 'model.decoder.layers.1.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.1.cross_attn.query.weight': 'model.decoder.layers.1.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.1.cross_attn.query.bias': 'model.decoder.layers.1.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.1.cross_attn.key.weight': 'model.decoder.layers.1.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.1.cross_attn.value.weight': 'model.decoder.layers.1.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.1.cross_attn.value.bias': 'model.decoder.layers.1.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.1.cross_attn.out.weight': 'model.decoder.layers.1.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.1.cross_attn.out.bias': 'model.decoder.layers.1.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.1.cross_attn_ln.weight': 'model.decoder.layers.1.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.1.cross_attn_ln.bias': 'model.decoder.layers.1.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.1.mlp.1.weight': 'model.decoder.layers.1.fc1.weight',\n",
       " 'decoder.blocks.1.mlp.1.bias': 'model.decoder.layers.1.fc1.bias',\n",
       " 'decoder.blocks.1.mlp.2.weight': 'model.decoder.layers.1.fc2.weight',\n",
       " 'decoder.blocks.1.mlp.2.bias': 'model.decoder.layers.1.fc2.bias',\n",
       " 'decoder.blocks.1.mlp_ln.weight': 'model.decoder.layers.1.final_layer_norm.weight',\n",
       " 'decoder.blocks.1.mlp_ln.bias': 'model.decoder.layers.1.final_layer_norm.bias',\n",
       " 'decoder.blocks.2.attn.query.weight': 'model.decoder.layers.2.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.2.attn.query.bias': 'model.decoder.layers.2.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.2.attn.key.weight': 'model.decoder.layers.2.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.2.attn.value.weight': 'model.decoder.layers.2.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.2.attn.value.bias': 'model.decoder.layers.2.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.2.attn.out.weight': 'model.decoder.layers.2.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.2.attn.out.bias': 'model.decoder.layers.2.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.2.attn_ln.weight': 'model.decoder.layers.2.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.2.attn_ln.bias': 'model.decoder.layers.2.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.2.cross_attn.query.weight': 'model.decoder.layers.2.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.2.cross_attn.query.bias': 'model.decoder.layers.2.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.2.cross_attn.key.weight': 'model.decoder.layers.2.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.2.cross_attn.value.weight': 'model.decoder.layers.2.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.2.cross_attn.value.bias': 'model.decoder.layers.2.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.2.cross_attn.out.weight': 'model.decoder.layers.2.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.2.cross_attn.out.bias': 'model.decoder.layers.2.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.2.cross_attn_ln.weight': 'model.decoder.layers.2.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.2.cross_attn_ln.bias': 'model.decoder.layers.2.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.2.mlp.2.weight': 'model.decoder.layers.2.fc2.weight',\n",
       " 'decoder.blocks.2.mlp.2.bias': 'model.decoder.layers.2.fc2.bias',\n",
       " 'decoder.blocks.2.mlp_ln.weight': 'model.decoder.layers.2.final_layer_norm.weight',\n",
       " 'decoder.blocks.2.mlp_ln.bias': 'model.decoder.layers.2.final_layer_norm.bias',\n",
       " 'decoder.blocks.3.attn.query.weight': 'model.decoder.layers.3.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.3.attn.query.bias': 'model.decoder.layers.3.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.3.attn.key.weight': 'model.decoder.layers.3.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.3.attn.value.weight': 'model.decoder.layers.3.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.3.attn.value.bias': 'model.decoder.layers.3.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.3.attn.out.weight': 'model.decoder.layers.3.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.3.attn.out.bias': 'model.decoder.layers.3.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.3.attn_ln.weight': 'model.decoder.layers.3.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.3.attn_ln.bias': 'model.decoder.layers.3.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.3.cross_attn.query.weight': 'model.decoder.layers.3.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.3.cross_attn.query.bias': 'model.decoder.layers.3.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.3.cross_attn.key.weight': 'model.decoder.layers.3.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.3.cross_attn.value.weight': 'model.decoder.layers.3.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.3.cross_attn.value.bias': 'model.decoder.layers.3.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.3.cross_attn.out.weight': 'model.decoder.layers.3.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.3.cross_attn.out.bias': 'model.decoder.layers.3.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.3.cross_attn_ln.weight': 'model.decoder.layers.3.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.3.cross_attn_ln.bias': 'model.decoder.layers.3.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.3.mlp.3.weight': 'model.decoder.layers.3.fc1.weight',\n",
       " 'decoder.blocks.3.mlp.3.bias': 'model.decoder.layers.3.fc1.bias',\n",
       " 'decoder.blocks.3.mlp.2.weight': 'model.decoder.layers.3.fc2.weight',\n",
       " 'decoder.blocks.3.mlp.2.bias': 'model.decoder.layers.3.fc2.bias',\n",
       " 'decoder.blocks.3.mlp_ln.weight': 'model.decoder.layers.3.final_layer_norm.weight',\n",
       " 'decoder.blocks.3.mlp_ln.bias': 'model.decoder.layers.3.final_layer_norm.bias',\n",
       " 'decoder.blocks.4.attn.query.weight': 'model.decoder.layers.4.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.4.attn.query.bias': 'model.decoder.layers.4.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.4.attn.key.weight': 'model.decoder.layers.4.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.4.attn.value.weight': 'model.decoder.layers.4.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.4.attn.value.bias': 'model.decoder.layers.4.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.4.attn.out.weight': 'model.decoder.layers.4.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.4.attn.out.bias': 'model.decoder.layers.4.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.4.attn_ln.weight': 'model.decoder.layers.4.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.4.attn_ln.bias': 'model.decoder.layers.4.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.4.cross_attn.query.weight': 'model.decoder.layers.4.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.4.cross_attn.query.bias': 'model.decoder.layers.4.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.4.cross_attn.key.weight': 'model.decoder.layers.4.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.4.cross_attn.value.weight': 'model.decoder.layers.4.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.4.cross_attn.value.bias': 'model.decoder.layers.4.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.4.cross_attn.out.weight': 'model.decoder.layers.4.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.4.cross_attn.out.bias': 'model.decoder.layers.4.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.4.cross_attn_ln.weight': 'model.decoder.layers.4.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.4.cross_attn_ln.bias': 'model.decoder.layers.4.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.4.mlp.4.weight': 'model.decoder.layers.4.fc1.weight',\n",
       " 'decoder.blocks.4.mlp.4.bias': 'model.decoder.layers.4.fc1.bias',\n",
       " 'decoder.blocks.4.mlp.2.weight': 'model.decoder.layers.4.fc2.weight',\n",
       " 'decoder.blocks.4.mlp.2.bias': 'model.decoder.layers.4.fc2.bias',\n",
       " 'decoder.blocks.4.mlp_ln.weight': 'model.decoder.layers.4.final_layer_norm.weight',\n",
       " 'decoder.blocks.4.mlp_ln.bias': 'model.decoder.layers.4.final_layer_norm.bias',\n",
       " 'decoder.blocks.5.attn.query.weight': 'model.decoder.layers.5.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.5.attn.query.bias': 'model.decoder.layers.5.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.5.attn.key.weight': 'model.decoder.layers.5.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.5.attn.value.weight': 'model.decoder.layers.5.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.5.attn.value.bias': 'model.decoder.layers.5.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.5.attn.out.weight': 'model.decoder.layers.5.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.5.attn.out.bias': 'model.decoder.layers.5.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.5.attn_ln.weight': 'model.decoder.layers.5.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.5.attn_ln.bias': 'model.decoder.layers.5.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.5.cross_attn.query.weight': 'model.decoder.layers.5.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.5.cross_attn.query.bias': 'model.decoder.layers.5.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.5.cross_attn.key.weight': 'model.decoder.layers.5.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.5.cross_attn.value.weight': 'model.decoder.layers.5.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.5.cross_attn.value.bias': 'model.decoder.layers.5.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.5.cross_attn.out.weight': 'model.decoder.layers.5.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.5.cross_attn.out.bias': 'model.decoder.layers.5.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.5.cross_attn_ln.weight': 'model.decoder.layers.5.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.5.cross_attn_ln.bias': 'model.decoder.layers.5.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.5.mlp.5.weight': 'model.decoder.layers.5.fc1.weight',\n",
       " 'decoder.blocks.5.mlp.5.bias': 'model.decoder.layers.5.fc1.bias',\n",
       " 'decoder.blocks.5.mlp.2.weight': 'model.decoder.layers.5.fc2.weight',\n",
       " 'decoder.blocks.5.mlp.2.bias': 'model.decoder.layers.5.fc2.bias',\n",
       " 'decoder.blocks.5.mlp_ln.weight': 'model.decoder.layers.5.final_layer_norm.weight',\n",
       " 'decoder.blocks.5.mlp_ln.bias': 'model.decoder.layers.5.final_layer_norm.bias',\n",
       " 'decoder.blocks.6.attn.query.weight': 'model.decoder.layers.6.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.6.attn.query.bias': 'model.decoder.layers.6.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.6.attn.key.weight': 'model.decoder.layers.6.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.6.attn.value.weight': 'model.decoder.layers.6.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.6.attn.value.bias': 'model.decoder.layers.6.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.6.attn.out.weight': 'model.decoder.layers.6.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.6.attn.out.bias': 'model.decoder.layers.6.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.6.attn_ln.weight': 'model.decoder.layers.6.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.6.attn_ln.bias': 'model.decoder.layers.6.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.6.cross_attn.query.weight': 'model.decoder.layers.6.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.6.cross_attn.query.bias': 'model.decoder.layers.6.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.6.cross_attn.key.weight': 'model.decoder.layers.6.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.6.cross_attn.value.weight': 'model.decoder.layers.6.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.6.cross_attn.value.bias': 'model.decoder.layers.6.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.6.cross_attn.out.weight': 'model.decoder.layers.6.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.6.cross_attn.out.bias': 'model.decoder.layers.6.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.6.cross_attn_ln.weight': 'model.decoder.layers.6.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.6.cross_attn_ln.bias': 'model.decoder.layers.6.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.6.mlp.6.weight': 'model.decoder.layers.6.fc1.weight',\n",
       " 'decoder.blocks.6.mlp.6.bias': 'model.decoder.layers.6.fc1.bias',\n",
       " 'decoder.blocks.6.mlp.2.weight': 'model.decoder.layers.6.fc2.weight',\n",
       " 'decoder.blocks.6.mlp.2.bias': 'model.decoder.layers.6.fc2.bias',\n",
       " 'decoder.blocks.6.mlp_ln.weight': 'model.decoder.layers.6.final_layer_norm.weight',\n",
       " 'decoder.blocks.6.mlp_ln.bias': 'model.decoder.layers.6.final_layer_norm.bias',\n",
       " 'decoder.blocks.7.attn.query.weight': 'model.decoder.layers.7.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.7.attn.query.bias': 'model.decoder.layers.7.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.7.attn.key.weight': 'model.decoder.layers.7.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.7.attn.value.weight': 'model.decoder.layers.7.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.7.attn.value.bias': 'model.decoder.layers.7.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.7.attn.out.weight': 'model.decoder.layers.7.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.7.attn.out.bias': 'model.decoder.layers.7.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.7.attn_ln.weight': 'model.decoder.layers.7.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.7.attn_ln.bias': 'model.decoder.layers.7.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.7.cross_attn.query.weight': 'model.decoder.layers.7.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.7.cross_attn.query.bias': 'model.decoder.layers.7.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.7.cross_attn.key.weight': 'model.decoder.layers.7.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.7.cross_attn.value.weight': 'model.decoder.layers.7.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.7.cross_attn.value.bias': 'model.decoder.layers.7.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.7.cross_attn.out.weight': 'model.decoder.layers.7.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.7.cross_attn.out.bias': 'model.decoder.layers.7.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.7.cross_attn_ln.weight': 'model.decoder.layers.7.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.7.cross_attn_ln.bias': 'model.decoder.layers.7.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.7.mlp.7.weight': 'model.decoder.layers.7.fc1.weight',\n",
       " 'decoder.blocks.7.mlp.7.bias': 'model.decoder.layers.7.fc1.bias',\n",
       " 'decoder.blocks.7.mlp.2.weight': 'model.decoder.layers.7.fc2.weight',\n",
       " 'decoder.blocks.7.mlp.2.bias': 'model.decoder.layers.7.fc2.bias',\n",
       " 'decoder.blocks.7.mlp_ln.weight': 'model.decoder.layers.7.final_layer_norm.weight',\n",
       " 'decoder.blocks.7.mlp_ln.bias': 'model.decoder.layers.7.final_layer_norm.bias',\n",
       " 'decoder.blocks.8.attn.query.weight': 'model.decoder.layers.8.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.8.attn.query.bias': 'model.decoder.layers.8.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.8.attn.key.weight': 'model.decoder.layers.8.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.8.attn.value.weight': 'model.decoder.layers.8.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.8.attn.value.bias': 'model.decoder.layers.8.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.8.attn.out.weight': 'model.decoder.layers.8.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.8.attn.out.bias': 'model.decoder.layers.8.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.8.attn_ln.weight': 'model.decoder.layers.8.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.8.attn_ln.bias': 'model.decoder.layers.8.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.8.cross_attn.query.weight': 'model.decoder.layers.8.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.8.cross_attn.query.bias': 'model.decoder.layers.8.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.8.cross_attn.key.weight': 'model.decoder.layers.8.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.8.cross_attn.value.weight': 'model.decoder.layers.8.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.8.cross_attn.value.bias': 'model.decoder.layers.8.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.8.cross_attn.out.weight': 'model.decoder.layers.8.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.8.cross_attn.out.bias': 'model.decoder.layers.8.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.8.cross_attn_ln.weight': 'model.decoder.layers.8.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.8.cross_attn_ln.bias': 'model.decoder.layers.8.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.8.mlp.8.weight': 'model.decoder.layers.8.fc1.weight',\n",
       " 'decoder.blocks.8.mlp.8.bias': 'model.decoder.layers.8.fc1.bias',\n",
       " 'decoder.blocks.8.mlp.2.weight': 'model.decoder.layers.8.fc2.weight',\n",
       " 'decoder.blocks.8.mlp.2.bias': 'model.decoder.layers.8.fc2.bias',\n",
       " 'decoder.blocks.8.mlp_ln.weight': 'model.decoder.layers.8.final_layer_norm.weight',\n",
       " 'decoder.blocks.8.mlp_ln.bias': 'model.decoder.layers.8.final_layer_norm.bias',\n",
       " 'decoder.blocks.9.attn.query.weight': 'model.decoder.layers.9.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.9.attn.query.bias': 'model.decoder.layers.9.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.9.attn.key.weight': 'model.decoder.layers.9.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.9.attn.value.weight': 'model.decoder.layers.9.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.9.attn.value.bias': 'model.decoder.layers.9.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.9.attn.out.weight': 'model.decoder.layers.9.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.9.attn.out.bias': 'model.decoder.layers.9.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.9.attn_ln.weight': 'model.decoder.layers.9.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.9.attn_ln.bias': 'model.decoder.layers.9.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.9.cross_attn.query.weight': 'model.decoder.layers.9.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.9.cross_attn.query.bias': 'model.decoder.layers.9.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.9.cross_attn.key.weight': 'model.decoder.layers.9.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.9.cross_attn.value.weight': 'model.decoder.layers.9.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.9.cross_attn.value.bias': 'model.decoder.layers.9.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.9.cross_attn.out.weight': 'model.decoder.layers.9.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.9.cross_attn.out.bias': 'model.decoder.layers.9.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.9.cross_attn_ln.weight': 'model.decoder.layers.9.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.9.cross_attn_ln.bias': 'model.decoder.layers.9.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.9.mlp.9.weight': 'model.decoder.layers.9.fc1.weight',\n",
       " 'decoder.blocks.9.mlp.9.bias': 'model.decoder.layers.9.fc1.bias',\n",
       " 'decoder.blocks.9.mlp.2.weight': 'model.decoder.layers.9.fc2.weight',\n",
       " 'decoder.blocks.9.mlp.2.bias': 'model.decoder.layers.9.fc2.bias',\n",
       " 'decoder.blocks.9.mlp_ln.weight': 'model.decoder.layers.9.final_layer_norm.weight',\n",
       " 'decoder.blocks.9.mlp_ln.bias': 'model.decoder.layers.9.final_layer_norm.bias',\n",
       " 'decoder.blocks.10.attn.query.weight': 'model.decoder.layers.10.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.10.attn.query.bias': 'model.decoder.layers.10.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.10.attn.key.weight': 'model.decoder.layers.10.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.10.attn.value.weight': 'model.decoder.layers.10.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.10.attn.value.bias': 'model.decoder.layers.10.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.10.attn.out.weight': 'model.decoder.layers.10.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.10.attn.out.bias': 'model.decoder.layers.10.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.10.attn_ln.weight': 'model.decoder.layers.10.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.10.attn_ln.bias': 'model.decoder.layers.10.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.10.cross_attn.query.weight': 'model.decoder.layers.10.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.10.cross_attn.query.bias': 'model.decoder.layers.10.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.10.cross_attn.key.weight': 'model.decoder.layers.10.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.10.cross_attn.value.weight': 'model.decoder.layers.10.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.10.cross_attn.value.bias': 'model.decoder.layers.10.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.10.cross_attn.out.weight': 'model.decoder.layers.10.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.10.cross_attn.out.bias': 'model.decoder.layers.10.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.10.cross_attn_ln.weight': 'model.decoder.layers.10.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.10.cross_attn_ln.bias': 'model.decoder.layers.10.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.10.mlp.10.weight': 'model.decoder.layers.10.fc1.weight',\n",
       " 'decoder.blocks.10.mlp.10.bias': 'model.decoder.layers.10.fc1.bias',\n",
       " 'decoder.blocks.10.mlp.2.weight': 'model.decoder.layers.10.fc2.weight',\n",
       " 'decoder.blocks.10.mlp.2.bias': 'model.decoder.layers.10.fc2.bias',\n",
       " 'decoder.blocks.10.mlp_ln.weight': 'model.decoder.layers.10.final_layer_norm.weight',\n",
       " 'decoder.blocks.10.mlp_ln.bias': 'model.decoder.layers.10.final_layer_norm.bias',\n",
       " 'decoder.blocks.11.attn.query.weight': 'model.decoder.layers.11.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.11.attn.query.bias': 'model.decoder.layers.11.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.11.attn.key.weight': 'model.decoder.layers.11.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.11.attn.value.weight': 'model.decoder.layers.11.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.11.attn.value.bias': 'model.decoder.layers.11.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.11.attn.out.weight': 'model.decoder.layers.11.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.11.attn.out.bias': 'model.decoder.layers.11.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.11.attn_ln.weight': 'model.decoder.layers.11.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.11.attn_ln.bias': 'model.decoder.layers.11.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.11.cross_attn.query.weight': 'model.decoder.layers.11.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.11.cross_attn.query.bias': 'model.decoder.layers.11.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.11.cross_attn.key.weight': 'model.decoder.layers.11.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.11.cross_attn.value.weight': 'model.decoder.layers.11.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.11.cross_attn.value.bias': 'model.decoder.layers.11.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.11.cross_attn.out.weight': 'model.decoder.layers.11.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.11.cross_attn.out.bias': 'model.decoder.layers.11.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.11.cross_attn_ln.weight': 'model.decoder.layers.11.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.11.cross_attn_ln.bias': 'model.decoder.layers.11.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.11.mlp.11.weight': 'model.decoder.layers.11.fc1.weight',\n",
       " 'decoder.blocks.11.mlp.11.bias': 'model.decoder.layers.11.fc1.bias',\n",
       " 'decoder.blocks.11.mlp.2.weight': 'model.decoder.layers.11.fc2.weight',\n",
       " 'decoder.blocks.11.mlp.2.bias': 'model.decoder.layers.11.fc2.bias',\n",
       " 'decoder.blocks.11.mlp_ln.weight': 'model.decoder.layers.11.final_layer_norm.weight',\n",
       " 'decoder.blocks.11.mlp_ln.bias': 'model.decoder.layers.11.final_layer_norm.bias',\n",
       " 'decoder.blocks.12.attn.query.weight': 'model.decoder.layers.12.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.12.attn.query.bias': 'model.decoder.layers.12.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.12.attn.key.weight': 'model.decoder.layers.12.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.12.attn.value.weight': 'model.decoder.layers.12.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.12.attn.value.bias': 'model.decoder.layers.12.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.12.attn.out.weight': 'model.decoder.layers.12.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.12.attn.out.bias': 'model.decoder.layers.12.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.12.attn_ln.weight': 'model.decoder.layers.12.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.12.attn_ln.bias': 'model.decoder.layers.12.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.12.cross_attn.query.weight': 'model.decoder.layers.12.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.12.cross_attn.query.bias': 'model.decoder.layers.12.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.12.cross_attn.key.weight': 'model.decoder.layers.12.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.12.cross_attn.value.weight': 'model.decoder.layers.12.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.12.cross_attn.value.bias': 'model.decoder.layers.12.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.12.cross_attn.out.weight': 'model.decoder.layers.12.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.12.cross_attn.out.bias': 'model.decoder.layers.12.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.12.cross_attn_ln.weight': 'model.decoder.layers.12.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.12.cross_attn_ln.bias': 'model.decoder.layers.12.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.12.mlp.12.weight': 'model.decoder.layers.12.fc1.weight',\n",
       " 'decoder.blocks.12.mlp.12.bias': 'model.decoder.layers.12.fc1.bias',\n",
       " 'decoder.blocks.12.mlp.2.weight': 'model.decoder.layers.12.fc2.weight',\n",
       " 'decoder.blocks.12.mlp.2.bias': 'model.decoder.layers.12.fc2.bias',\n",
       " 'decoder.blocks.12.mlp_ln.weight': 'model.decoder.layers.12.final_layer_norm.weight',\n",
       " 'decoder.blocks.12.mlp_ln.bias': 'model.decoder.layers.12.final_layer_norm.bias',\n",
       " 'decoder.blocks.13.attn.query.weight': 'model.decoder.layers.13.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.13.attn.query.bias': 'model.decoder.layers.13.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.13.attn.key.weight': 'model.decoder.layers.13.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.13.attn.value.weight': 'model.decoder.layers.13.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.13.attn.value.bias': 'model.decoder.layers.13.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.13.attn.out.weight': 'model.decoder.layers.13.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.13.attn.out.bias': 'model.decoder.layers.13.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.13.attn_ln.weight': 'model.decoder.layers.13.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.13.attn_ln.bias': 'model.decoder.layers.13.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.13.cross_attn.query.weight': 'model.decoder.layers.13.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.13.cross_attn.query.bias': 'model.decoder.layers.13.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.13.cross_attn.key.weight': 'model.decoder.layers.13.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.13.cross_attn.value.weight': 'model.decoder.layers.13.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.13.cross_attn.value.bias': 'model.decoder.layers.13.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.13.cross_attn.out.weight': 'model.decoder.layers.13.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.13.cross_attn.out.bias': 'model.decoder.layers.13.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.13.cross_attn_ln.weight': 'model.decoder.layers.13.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.13.cross_attn_ln.bias': 'model.decoder.layers.13.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.13.mlp.13.weight': 'model.decoder.layers.13.fc1.weight',\n",
       " 'decoder.blocks.13.mlp.13.bias': 'model.decoder.layers.13.fc1.bias',\n",
       " 'decoder.blocks.13.mlp.2.weight': 'model.decoder.layers.13.fc2.weight',\n",
       " 'decoder.blocks.13.mlp.2.bias': 'model.decoder.layers.13.fc2.bias',\n",
       " 'decoder.blocks.13.mlp_ln.weight': 'model.decoder.layers.13.final_layer_norm.weight',\n",
       " 'decoder.blocks.13.mlp_ln.bias': 'model.decoder.layers.13.final_layer_norm.bias',\n",
       " 'decoder.blocks.14.attn.query.weight': 'model.decoder.layers.14.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.14.attn.query.bias': 'model.decoder.layers.14.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.14.attn.key.weight': 'model.decoder.layers.14.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.14.attn.value.weight': 'model.decoder.layers.14.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.14.attn.value.bias': 'model.decoder.layers.14.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.14.attn.out.weight': 'model.decoder.layers.14.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.14.attn.out.bias': 'model.decoder.layers.14.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.14.attn_ln.weight': 'model.decoder.layers.14.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.14.attn_ln.bias': 'model.decoder.layers.14.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.14.cross_attn.query.weight': 'model.decoder.layers.14.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.14.cross_attn.query.bias': 'model.decoder.layers.14.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.14.cross_attn.key.weight': 'model.decoder.layers.14.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.14.cross_attn.value.weight': 'model.decoder.layers.14.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.14.cross_attn.value.bias': 'model.decoder.layers.14.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.14.cross_attn.out.weight': 'model.decoder.layers.14.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.14.cross_attn.out.bias': 'model.decoder.layers.14.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.14.cross_attn_ln.weight': 'model.decoder.layers.14.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.14.cross_attn_ln.bias': 'model.decoder.layers.14.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.14.mlp.14.weight': 'model.decoder.layers.14.fc1.weight',\n",
       " 'decoder.blocks.14.mlp.14.bias': 'model.decoder.layers.14.fc1.bias',\n",
       " 'decoder.blocks.14.mlp.2.weight': 'model.decoder.layers.14.fc2.weight',\n",
       " 'decoder.blocks.14.mlp.2.bias': 'model.decoder.layers.14.fc2.bias',\n",
       " 'decoder.blocks.14.mlp_ln.weight': 'model.decoder.layers.14.final_layer_norm.weight',\n",
       " 'decoder.blocks.14.mlp_ln.bias': 'model.decoder.layers.14.final_layer_norm.bias',\n",
       " 'decoder.blocks.15.attn.query.weight': 'model.decoder.layers.15.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.15.attn.query.bias': 'model.decoder.layers.15.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.15.attn.key.weight': 'model.decoder.layers.15.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.15.attn.value.weight': 'model.decoder.layers.15.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.15.attn.value.bias': 'model.decoder.layers.15.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.15.attn.out.weight': 'model.decoder.layers.15.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.15.attn.out.bias': 'model.decoder.layers.15.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.15.attn_ln.weight': 'model.decoder.layers.15.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.15.attn_ln.bias': 'model.decoder.layers.15.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.15.cross_attn.query.weight': 'model.decoder.layers.15.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.15.cross_attn.query.bias': 'model.decoder.layers.15.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.15.cross_attn.key.weight': 'model.decoder.layers.15.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.15.cross_attn.value.weight': 'model.decoder.layers.15.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.15.cross_attn.value.bias': 'model.decoder.layers.15.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.15.cross_attn.out.weight': 'model.decoder.layers.15.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.15.cross_attn.out.bias': 'model.decoder.layers.15.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.15.cross_attn_ln.weight': 'model.decoder.layers.15.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.15.cross_attn_ln.bias': 'model.decoder.layers.15.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.15.mlp.15.weight': 'model.decoder.layers.15.fc1.weight',\n",
       " 'decoder.blocks.15.mlp.15.bias': 'model.decoder.layers.15.fc1.bias',\n",
       " 'decoder.blocks.15.mlp.2.weight': 'model.decoder.layers.15.fc2.weight',\n",
       " 'decoder.blocks.15.mlp.2.bias': 'model.decoder.layers.15.fc2.bias',\n",
       " 'decoder.blocks.15.mlp_ln.weight': 'model.decoder.layers.15.final_layer_norm.weight',\n",
       " 'decoder.blocks.15.mlp_ln.bias': 'model.decoder.layers.15.final_layer_norm.bias',\n",
       " 'decoder.blocks.16.attn.query.weight': 'model.decoder.layers.16.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.16.attn.query.bias': 'model.decoder.layers.16.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.16.attn.key.weight': 'model.decoder.layers.16.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.16.attn.value.weight': 'model.decoder.layers.16.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.16.attn.value.bias': 'model.decoder.layers.16.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.16.attn.out.weight': 'model.decoder.layers.16.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.16.attn.out.bias': 'model.decoder.layers.16.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.16.attn_ln.weight': 'model.decoder.layers.16.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.16.attn_ln.bias': 'model.decoder.layers.16.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.16.cross_attn.query.weight': 'model.decoder.layers.16.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.16.cross_attn.query.bias': 'model.decoder.layers.16.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.16.cross_attn.key.weight': 'model.decoder.layers.16.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.16.cross_attn.value.weight': 'model.decoder.layers.16.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.16.cross_attn.value.bias': 'model.decoder.layers.16.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.16.cross_attn.out.weight': 'model.decoder.layers.16.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.16.cross_attn.out.bias': 'model.decoder.layers.16.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.16.cross_attn_ln.weight': 'model.decoder.layers.16.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.16.cross_attn_ln.bias': 'model.decoder.layers.16.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.16.mlp.16.weight': 'model.decoder.layers.16.fc1.weight',\n",
       " 'decoder.blocks.16.mlp.16.bias': 'model.decoder.layers.16.fc1.bias',\n",
       " 'decoder.blocks.16.mlp.2.weight': 'model.decoder.layers.16.fc2.weight',\n",
       " 'decoder.blocks.16.mlp.2.bias': 'model.decoder.layers.16.fc2.bias',\n",
       " 'decoder.blocks.16.mlp_ln.weight': 'model.decoder.layers.16.final_layer_norm.weight',\n",
       " 'decoder.blocks.16.mlp_ln.bias': 'model.decoder.layers.16.final_layer_norm.bias',\n",
       " 'decoder.blocks.17.attn.query.weight': 'model.decoder.layers.17.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.17.attn.query.bias': 'model.decoder.layers.17.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.17.attn.key.weight': 'model.decoder.layers.17.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.17.attn.value.weight': 'model.decoder.layers.17.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.17.attn.value.bias': 'model.decoder.layers.17.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.17.attn.out.weight': 'model.decoder.layers.17.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.17.attn.out.bias': 'model.decoder.layers.17.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.17.attn_ln.weight': 'model.decoder.layers.17.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.17.attn_ln.bias': 'model.decoder.layers.17.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.17.cross_attn.query.weight': 'model.decoder.layers.17.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.17.cross_attn.query.bias': 'model.decoder.layers.17.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.17.cross_attn.key.weight': 'model.decoder.layers.17.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.17.cross_attn.value.weight': 'model.decoder.layers.17.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.17.cross_attn.value.bias': 'model.decoder.layers.17.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.17.cross_attn.out.weight': 'model.decoder.layers.17.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.17.cross_attn.out.bias': 'model.decoder.layers.17.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.17.cross_attn_ln.weight': 'model.decoder.layers.17.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.17.cross_attn_ln.bias': 'model.decoder.layers.17.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.17.mlp.17.weight': 'model.decoder.layers.17.fc1.weight',\n",
       " 'decoder.blocks.17.mlp.17.bias': 'model.decoder.layers.17.fc1.bias',\n",
       " 'decoder.blocks.17.mlp.2.weight': 'model.decoder.layers.17.fc2.weight',\n",
       " 'decoder.blocks.17.mlp.2.bias': 'model.decoder.layers.17.fc2.bias',\n",
       " 'decoder.blocks.17.mlp_ln.weight': 'model.decoder.layers.17.final_layer_norm.weight',\n",
       " 'decoder.blocks.17.mlp_ln.bias': 'model.decoder.layers.17.final_layer_norm.bias',\n",
       " 'decoder.blocks.18.attn.query.weight': 'model.decoder.layers.18.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.18.attn.query.bias': 'model.decoder.layers.18.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.18.attn.key.weight': 'model.decoder.layers.18.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.18.attn.value.weight': 'model.decoder.layers.18.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.18.attn.value.bias': 'model.decoder.layers.18.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.18.attn.out.weight': 'model.decoder.layers.18.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.18.attn.out.bias': 'model.decoder.layers.18.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.18.attn_ln.weight': 'model.decoder.layers.18.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.18.attn_ln.bias': 'model.decoder.layers.18.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.18.cross_attn.query.weight': 'model.decoder.layers.18.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.18.cross_attn.query.bias': 'model.decoder.layers.18.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.18.cross_attn.key.weight': 'model.decoder.layers.18.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.18.cross_attn.value.weight': 'model.decoder.layers.18.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.18.cross_attn.value.bias': 'model.decoder.layers.18.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.18.cross_attn.out.weight': 'model.decoder.layers.18.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.18.cross_attn.out.bias': 'model.decoder.layers.18.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.18.cross_attn_ln.weight': 'model.decoder.layers.18.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.18.cross_attn_ln.bias': 'model.decoder.layers.18.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.18.mlp.18.weight': 'model.decoder.layers.18.fc1.weight',\n",
       " 'decoder.blocks.18.mlp.18.bias': 'model.decoder.layers.18.fc1.bias',\n",
       " 'decoder.blocks.18.mlp.2.weight': 'model.decoder.layers.18.fc2.weight',\n",
       " 'decoder.blocks.18.mlp.2.bias': 'model.decoder.layers.18.fc2.bias',\n",
       " 'decoder.blocks.18.mlp_ln.weight': 'model.decoder.layers.18.final_layer_norm.weight',\n",
       " 'decoder.blocks.18.mlp_ln.bias': 'model.decoder.layers.18.final_layer_norm.bias',\n",
       " 'decoder.blocks.19.attn.query.weight': 'model.decoder.layers.19.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.19.attn.query.bias': 'model.decoder.layers.19.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.19.attn.key.weight': 'model.decoder.layers.19.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.19.attn.value.weight': 'model.decoder.layers.19.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.19.attn.value.bias': 'model.decoder.layers.19.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.19.attn.out.weight': 'model.decoder.layers.19.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.19.attn.out.bias': 'model.decoder.layers.19.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.19.attn_ln.weight': 'model.decoder.layers.19.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.19.attn_ln.bias': 'model.decoder.layers.19.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.19.cross_attn.query.weight': 'model.decoder.layers.19.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.19.cross_attn.query.bias': 'model.decoder.layers.19.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.19.cross_attn.key.weight': 'model.decoder.layers.19.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.19.cross_attn.value.weight': 'model.decoder.layers.19.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.19.cross_attn.value.bias': 'model.decoder.layers.19.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.19.cross_attn.out.weight': 'model.decoder.layers.19.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.19.cross_attn.out.bias': 'model.decoder.layers.19.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.19.cross_attn_ln.weight': 'model.decoder.layers.19.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.19.cross_attn_ln.bias': 'model.decoder.layers.19.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.19.mlp.19.weight': 'model.decoder.layers.19.fc1.weight',\n",
       " 'decoder.blocks.19.mlp.19.bias': 'model.decoder.layers.19.fc1.bias',\n",
       " 'decoder.blocks.19.mlp.2.weight': 'model.decoder.layers.19.fc2.weight',\n",
       " 'decoder.blocks.19.mlp.2.bias': 'model.decoder.layers.19.fc2.bias',\n",
       " 'decoder.blocks.19.mlp_ln.weight': 'model.decoder.layers.19.final_layer_norm.weight',\n",
       " 'decoder.blocks.19.mlp_ln.bias': 'model.decoder.layers.19.final_layer_norm.bias',\n",
       " 'decoder.blocks.20.attn.query.weight': 'model.decoder.layers.20.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.20.attn.query.bias': 'model.decoder.layers.20.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.20.attn.key.weight': 'model.decoder.layers.20.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.20.attn.value.weight': 'model.decoder.layers.20.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.20.attn.value.bias': 'model.decoder.layers.20.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.20.attn.out.weight': 'model.decoder.layers.20.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.20.attn.out.bias': 'model.decoder.layers.20.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.20.attn_ln.weight': 'model.decoder.layers.20.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.20.attn_ln.bias': 'model.decoder.layers.20.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.20.cross_attn.query.weight': 'model.decoder.layers.20.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.20.cross_attn.query.bias': 'model.decoder.layers.20.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.20.cross_attn.key.weight': 'model.decoder.layers.20.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.20.cross_attn.value.weight': 'model.decoder.layers.20.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.20.cross_attn.value.bias': 'model.decoder.layers.20.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.20.cross_attn.out.weight': 'model.decoder.layers.20.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.20.cross_attn.out.bias': 'model.decoder.layers.20.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.20.cross_attn_ln.weight': 'model.decoder.layers.20.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.20.cross_attn_ln.bias': 'model.decoder.layers.20.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.20.mlp.20.weight': 'model.decoder.layers.20.fc1.weight',\n",
       " 'decoder.blocks.20.mlp.20.bias': 'model.decoder.layers.20.fc1.bias',\n",
       " 'decoder.blocks.20.mlp.2.weight': 'model.decoder.layers.20.fc2.weight',\n",
       " 'decoder.blocks.20.mlp.2.bias': 'model.decoder.layers.20.fc2.bias',\n",
       " 'decoder.blocks.20.mlp_ln.weight': 'model.decoder.layers.20.final_layer_norm.weight',\n",
       " 'decoder.blocks.20.mlp_ln.bias': 'model.decoder.layers.20.final_layer_norm.bias',\n",
       " 'decoder.blocks.21.attn.query.weight': 'model.decoder.layers.21.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.21.attn.query.bias': 'model.decoder.layers.21.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.21.attn.key.weight': 'model.decoder.layers.21.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.21.attn.value.weight': 'model.decoder.layers.21.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.21.attn.value.bias': 'model.decoder.layers.21.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.21.attn.out.weight': 'model.decoder.layers.21.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.21.attn.out.bias': 'model.decoder.layers.21.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.21.attn_ln.weight': 'model.decoder.layers.21.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.21.attn_ln.bias': 'model.decoder.layers.21.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.21.cross_attn.query.weight': 'model.decoder.layers.21.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.21.cross_attn.query.bias': 'model.decoder.layers.21.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.21.cross_attn.key.weight': 'model.decoder.layers.21.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.21.cross_attn.value.weight': 'model.decoder.layers.21.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.21.cross_attn.value.bias': 'model.decoder.layers.21.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.21.cross_attn.out.weight': 'model.decoder.layers.21.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.21.cross_attn.out.bias': 'model.decoder.layers.21.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.21.cross_attn_ln.weight': 'model.decoder.layers.21.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.21.cross_attn_ln.bias': 'model.decoder.layers.21.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.21.mlp.21.weight': 'model.decoder.layers.21.fc1.weight',\n",
       " 'decoder.blocks.21.mlp.21.bias': 'model.decoder.layers.21.fc1.bias',\n",
       " 'decoder.blocks.21.mlp.2.weight': 'model.decoder.layers.21.fc2.weight',\n",
       " 'decoder.blocks.21.mlp.2.bias': 'model.decoder.layers.21.fc2.bias',\n",
       " 'decoder.blocks.21.mlp_ln.weight': 'model.decoder.layers.21.final_layer_norm.weight',\n",
       " 'decoder.blocks.21.mlp_ln.bias': 'model.decoder.layers.21.final_layer_norm.bias',\n",
       " 'decoder.blocks.22.attn.query.weight': 'model.decoder.layers.22.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.22.attn.query.bias': 'model.decoder.layers.22.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.22.attn.key.weight': 'model.decoder.layers.22.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.22.attn.value.weight': 'model.decoder.layers.22.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.22.attn.value.bias': 'model.decoder.layers.22.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.22.attn.out.weight': 'model.decoder.layers.22.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.22.attn.out.bias': 'model.decoder.layers.22.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.22.attn_ln.weight': 'model.decoder.layers.22.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.22.attn_ln.bias': 'model.decoder.layers.22.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.22.cross_attn.query.weight': 'model.decoder.layers.22.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.22.cross_attn.query.bias': 'model.decoder.layers.22.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.22.cross_attn.key.weight': 'model.decoder.layers.22.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.22.cross_attn.value.weight': 'model.decoder.layers.22.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.22.cross_attn.value.bias': 'model.decoder.layers.22.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.22.cross_attn.out.weight': 'model.decoder.layers.22.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.22.cross_attn.out.bias': 'model.decoder.layers.22.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.22.cross_attn_ln.weight': 'model.decoder.layers.22.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.22.cross_attn_ln.bias': 'model.decoder.layers.22.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.22.mlp.22.weight': 'model.decoder.layers.22.fc1.weight',\n",
       " 'decoder.blocks.22.mlp.22.bias': 'model.decoder.layers.22.fc1.bias',\n",
       " 'decoder.blocks.22.mlp.2.weight': 'model.decoder.layers.22.fc2.weight',\n",
       " 'decoder.blocks.22.mlp.2.bias': 'model.decoder.layers.22.fc2.bias',\n",
       " 'decoder.blocks.22.mlp_ln.weight': 'model.decoder.layers.22.final_layer_norm.weight',\n",
       " 'decoder.blocks.22.mlp_ln.bias': 'model.decoder.layers.22.final_layer_norm.bias',\n",
       " 'decoder.blocks.23.attn.query.weight': 'model.decoder.layers.23.self_attn.q_proj.weight',\n",
       " 'decoder.blocks.23.attn.query.bias': 'model.decoder.layers.23.self_attn.q_proj.bias',\n",
       " 'decoder.blocks.23.attn.key.weight': 'model.decoder.layers.23.self_attn.k_proj.weight',\n",
       " 'decoder.blocks.23.attn.value.weight': 'model.decoder.layers.23.self_attn.v_proj.weight',\n",
       " 'decoder.blocks.23.attn.value.bias': 'model.decoder.layers.23.self_attn.v_proj.bias',\n",
       " 'decoder.blocks.23.attn.out.weight': 'model.decoder.layers.23.self_attn.out_proj.weight',\n",
       " 'decoder.blocks.23.attn.out.bias': 'model.decoder.layers.23.self_attn.out_proj.bias',\n",
       " 'decoder.blocks.23.attn_ln.weight': 'model.decoder.layers.23.self_attn_layer_norm.weight',\n",
       " 'decoder.blocks.23.attn_ln.bias': 'model.decoder.layers.23.self_attn_layer_norm.bias',\n",
       " 'decoder.blocks.23.cross_attn.query.weight': 'model.decoder.layers.23.encoder_attn.q_proj.weight',\n",
       " 'decoder.blocks.23.cross_attn.query.bias': 'model.decoder.layers.23.encoder_attn.q_proj.bias',\n",
       " 'decoder.blocks.23.cross_attn.key.weight': 'model.decoder.layers.23.encoder_attn.k_proj.weight',\n",
       " 'decoder.blocks.23.cross_attn.value.weight': 'model.decoder.layers.23.encoder_attn.v_proj.weight',\n",
       " 'decoder.blocks.23.cross_attn.value.bias': 'model.decoder.layers.23.encoder_attn.v_proj.bias',\n",
       " 'decoder.blocks.23.cross_attn.out.weight': 'model.decoder.layers.23.encoder_attn.out_proj.weight',\n",
       " 'decoder.blocks.23.cross_attn.out.bias': 'model.decoder.layers.23.encoder_attn.out_proj.bias',\n",
       " 'decoder.blocks.23.cross_attn_ln.weight': 'model.decoder.layers.23.encoder_attn_layer_norm.weight',\n",
       " 'decoder.blocks.23.cross_attn_ln.bias': 'model.decoder.layers.23.encoder_attn_layer_norm.bias',\n",
       " 'decoder.blocks.23.mlp.23.weight': 'model.decoder.layers.23.fc1.weight',\n",
       " 'decoder.blocks.23.mlp.23.bias': 'model.decoder.layers.23.fc1.bias',\n",
       " 'decoder.blocks.23.mlp.2.weight': 'model.decoder.layers.23.fc2.weight',\n",
       " 'decoder.blocks.23.mlp.2.bias': 'model.decoder.layers.23.fc2.bias',\n",
       " 'decoder.blocks.23.mlp_ln.weight': 'model.decoder.layers.23.final_layer_norm.weight',\n",
       " 'decoder.blocks.23.mlp_ln.bias': 'model.decoder.layers.23.final_layer_norm.bias',\n",
       " 'decoder.ln.weight': 'model.decoder.layer_norm.weight',\n",
       " 'decoder.ln.bias': 'model.decoder.layer_norm.bias'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WHISPER_HF_MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-chain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
