{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762321920"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model('medium')\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51865, 1024])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.token_embedding.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "763857920"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model_hf = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")\n",
    "total_params = sum(p.numel() for p in model_hf.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-23): 24 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51865, 1024)\n",
       "    (blocks): ModuleList(\n",
       "      (0-23): 24 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 1024, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1024, out_features=51865, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model_hf.named_modules():\n",
    "    # print(name)\n",
    "    if name == 'proj_out':\n",
    "        proj_out = module\n",
    "    if name == 'model.decoder.embed_tokens':\n",
    "        embed_tokens = module\n",
    "    if name == 'model.encoder.layers.7.fc2':\n",
    "        fc1 = module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51865, 1024])\n",
      "Parameter containing:\n",
      "tensor([[-0.0026, -0.0016,  0.0062,  ...,  0.0007, -0.0092, -0.0009],\n",
      "        [-0.0114,  0.0123,  0.0081,  ..., -0.0173, -0.0045, -0.0050],\n",
      "        [ 0.0072, -0.0045, -0.0065,  ..., -0.0025,  0.0165, -0.0018],\n",
      "        ...,\n",
      "        [ 0.0015,  0.0010,  0.0010,  ..., -0.0027,  0.0107,  0.0038],\n",
      "        [ 0.0039,  0.0015,  0.0009,  ..., -0.0072,  0.0132,  0.0083],\n",
      "        [ 0.0097, -0.0006, -0.0127,  ...,  0.0075,  0.0166,  0.0010]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p1 in proj_out.parameters():\n",
    "    print(p1.size())\n",
    "    print(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51865, 1024])\n",
      "Parameter containing:\n",
      "tensor([[-0.0026, -0.0016,  0.0062,  ...,  0.0007, -0.0092, -0.0009],\n",
      "        [-0.0114,  0.0123,  0.0081,  ..., -0.0173, -0.0045, -0.0050],\n",
      "        [ 0.0072, -0.0045, -0.0065,  ..., -0.0025,  0.0165, -0.0018],\n",
      "        ...,\n",
      "        [ 0.0015,  0.0010,  0.0010,  ..., -0.0027,  0.0107,  0.0038],\n",
      "        [ 0.0039,  0.0015,  0.0009,  ..., -0.0072,  0.0132,  0.0083],\n",
      "        [ 0.0097, -0.0006, -0.0127,  ...,  0.0075,  0.0166,  0.0010]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p2 in embed_tokens.parameters():\n",
    "    print(p2.size())\n",
    "    print(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 4096])\n",
      "Parameter containing:\n",
      "tensor([[-0.0014,  0.0094, -0.0125,  ..., -0.0019, -0.0003,  0.0043],\n",
      "        [-0.0028, -0.0017,  0.0004,  ..., -0.0185,  0.0012, -0.0071],\n",
      "        [-0.0091,  0.0173,  0.0221,  ...,  0.0015, -0.0062,  0.0131],\n",
      "        ...,\n",
      "        [-0.0167,  0.0057, -0.0172,  ..., -0.0072,  0.0224, -0.0065],\n",
      "        [ 0.0151, -0.0184, -0.0029,  ..., -0.0014, -0.0027, -0.0003],\n",
      "        [-0.0319,  0.0036, -0.0266,  ..., -0.0122,  0.0123, -0.0112]],\n",
      "       requires_grad=True)\n",
      "torch.Size([1024])\n",
      "Parameter containing:\n",
      "tensor([ 0.0164,  0.0142, -0.0358,  ..., -0.0742,  0.0017, -0.0022],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p3 in fc1.parameters():\n",
    "    print(p3.size())\n",
    "    print(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "embed_tokens.parameters() == proj_out.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(proj_out.parameters()) == list(embed_tokens.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.conv1.weight\n",
      "encoder.conv1.bias\n",
      "encoder.conv2.weight\n",
      "encoder.conv2.bias\n",
      "encoder.blocks.0.attn.query.weight\n",
      "encoder.blocks.0.attn.query.bias\n",
      "encoder.blocks.0.attn.key.weight\n",
      "encoder.blocks.0.attn.value.weight\n",
      "encoder.blocks.0.attn.value.bias\n",
      "encoder.blocks.0.attn.out.weight\n",
      "encoder.blocks.0.attn.out.bias\n",
      "encoder.blocks.0.attn_ln.weight\n",
      "encoder.blocks.0.attn_ln.bias\n",
      "encoder.blocks.0.mlp.0.weight\n",
      "encoder.blocks.0.mlp.0.bias\n",
      "encoder.blocks.0.mlp.2.weight\n",
      "encoder.blocks.0.mlp.2.bias\n",
      "encoder.blocks.0.mlp_ln.weight\n",
      "encoder.blocks.0.mlp_ln.bias\n",
      "encoder.blocks.1.attn.query.weight\n",
      "encoder.blocks.1.attn.query.bias\n",
      "encoder.blocks.1.attn.key.weight\n",
      "encoder.blocks.1.attn.value.weight\n",
      "encoder.blocks.1.attn.value.bias\n",
      "encoder.blocks.1.attn.out.weight\n",
      "encoder.blocks.1.attn.out.bias\n",
      "encoder.blocks.1.attn_ln.weight\n",
      "encoder.blocks.1.attn_ln.bias\n",
      "encoder.blocks.1.mlp.0.weight\n",
      "encoder.blocks.1.mlp.0.bias\n",
      "encoder.blocks.1.mlp.2.weight\n",
      "encoder.blocks.1.mlp.2.bias\n",
      "encoder.blocks.1.mlp_ln.weight\n",
      "encoder.blocks.1.mlp_ln.bias\n",
      "encoder.blocks.2.attn.query.weight\n",
      "encoder.blocks.2.attn.query.bias\n",
      "encoder.blocks.2.attn.key.weight\n",
      "encoder.blocks.2.attn.value.weight\n",
      "encoder.blocks.2.attn.value.bias\n",
      "encoder.blocks.2.attn.out.weight\n",
      "encoder.blocks.2.attn.out.bias\n",
      "encoder.blocks.2.attn_ln.weight\n",
      "encoder.blocks.2.attn_ln.bias\n",
      "encoder.blocks.2.mlp.0.weight\n",
      "encoder.blocks.2.mlp.0.bias\n",
      "encoder.blocks.2.mlp.2.weight\n",
      "encoder.blocks.2.mlp.2.bias\n",
      "encoder.blocks.2.mlp_ln.weight\n",
      "encoder.blocks.2.mlp_ln.bias\n",
      "encoder.blocks.3.attn.query.weight\n",
      "encoder.blocks.3.attn.query.bias\n",
      "encoder.blocks.3.attn.key.weight\n",
      "encoder.blocks.3.attn.value.weight\n",
      "encoder.blocks.3.attn.value.bias\n",
      "encoder.blocks.3.attn.out.weight\n",
      "encoder.blocks.3.attn.out.bias\n",
      "encoder.blocks.3.attn_ln.weight\n",
      "encoder.blocks.3.attn_ln.bias\n",
      "encoder.blocks.3.mlp.0.weight\n",
      "encoder.blocks.3.mlp.0.bias\n",
      "encoder.blocks.3.mlp.2.weight\n",
      "encoder.blocks.3.mlp.2.bias\n",
      "encoder.blocks.3.mlp_ln.weight\n",
      "encoder.blocks.3.mlp_ln.bias\n",
      "encoder.blocks.4.attn.query.weight\n",
      "encoder.blocks.4.attn.query.bias\n",
      "encoder.blocks.4.attn.key.weight\n",
      "encoder.blocks.4.attn.value.weight\n",
      "encoder.blocks.4.attn.value.bias\n",
      "encoder.blocks.4.attn.out.weight\n",
      "encoder.blocks.4.attn.out.bias\n",
      "encoder.blocks.4.attn_ln.weight\n",
      "encoder.blocks.4.attn_ln.bias\n",
      "encoder.blocks.4.mlp.0.weight\n",
      "encoder.blocks.4.mlp.0.bias\n",
      "encoder.blocks.4.mlp.2.weight\n",
      "encoder.blocks.4.mlp.2.bias\n",
      "encoder.blocks.4.mlp_ln.weight\n",
      "encoder.blocks.4.mlp_ln.bias\n",
      "encoder.blocks.5.attn.query.weight\n",
      "encoder.blocks.5.attn.query.bias\n",
      "encoder.blocks.5.attn.key.weight\n",
      "encoder.blocks.5.attn.value.weight\n",
      "encoder.blocks.5.attn.value.bias\n",
      "encoder.blocks.5.attn.out.weight\n",
      "encoder.blocks.5.attn.out.bias\n",
      "encoder.blocks.5.attn_ln.weight\n",
      "encoder.blocks.5.attn_ln.bias\n",
      "encoder.blocks.5.mlp.0.weight\n",
      "encoder.blocks.5.mlp.0.bias\n",
      "encoder.blocks.5.mlp.2.weight\n",
      "encoder.blocks.5.mlp.2.bias\n",
      "encoder.blocks.5.mlp_ln.weight\n",
      "encoder.blocks.5.mlp_ln.bias\n",
      "encoder.blocks.6.attn.query.weight\n",
      "encoder.blocks.6.attn.query.bias\n",
      "encoder.blocks.6.attn.key.weight\n",
      "encoder.blocks.6.attn.value.weight\n",
      "encoder.blocks.6.attn.value.bias\n",
      "encoder.blocks.6.attn.out.weight\n",
      "encoder.blocks.6.attn.out.bias\n",
      "encoder.blocks.6.attn_ln.weight\n",
      "encoder.blocks.6.attn_ln.bias\n",
      "encoder.blocks.6.mlp.0.weight\n",
      "encoder.blocks.6.mlp.0.bias\n",
      "encoder.blocks.6.mlp.2.weight\n",
      "encoder.blocks.6.mlp.2.bias\n",
      "encoder.blocks.6.mlp_ln.weight\n",
      "encoder.blocks.6.mlp_ln.bias\n",
      "encoder.blocks.7.attn.query.weight\n",
      "encoder.blocks.7.attn.query.bias\n",
      "encoder.blocks.7.attn.key.weight\n",
      "encoder.blocks.7.attn.value.weight\n",
      "encoder.blocks.7.attn.value.bias\n",
      "encoder.blocks.7.attn.out.weight\n",
      "encoder.blocks.7.attn.out.bias\n",
      "encoder.blocks.7.attn_ln.weight\n",
      "encoder.blocks.7.attn_ln.bias\n",
      "encoder.blocks.7.mlp.0.weight\n",
      "encoder.blocks.7.mlp.0.bias\n",
      "encoder.blocks.7.mlp.2.weight\n",
      "encoder.blocks.7.mlp.2.bias\n",
      "encoder.blocks.7.mlp_ln.weight\n",
      "encoder.blocks.7.mlp_ln.bias\n",
      "encoder.blocks.8.attn.query.weight\n",
      "encoder.blocks.8.attn.query.bias\n",
      "encoder.blocks.8.attn.key.weight\n",
      "encoder.blocks.8.attn.value.weight\n",
      "encoder.blocks.8.attn.value.bias\n",
      "encoder.blocks.8.attn.out.weight\n",
      "encoder.blocks.8.attn.out.bias\n",
      "encoder.blocks.8.attn_ln.weight\n",
      "encoder.blocks.8.attn_ln.bias\n",
      "encoder.blocks.8.mlp.0.weight\n",
      "encoder.blocks.8.mlp.0.bias\n",
      "encoder.blocks.8.mlp.2.weight\n",
      "encoder.blocks.8.mlp.2.bias\n",
      "encoder.blocks.8.mlp_ln.weight\n",
      "encoder.blocks.8.mlp_ln.bias\n",
      "encoder.blocks.9.attn.query.weight\n",
      "encoder.blocks.9.attn.query.bias\n",
      "encoder.blocks.9.attn.key.weight\n",
      "encoder.blocks.9.attn.value.weight\n",
      "encoder.blocks.9.attn.value.bias\n",
      "encoder.blocks.9.attn.out.weight\n",
      "encoder.blocks.9.attn.out.bias\n",
      "encoder.blocks.9.attn_ln.weight\n",
      "encoder.blocks.9.attn_ln.bias\n",
      "encoder.blocks.9.mlp.0.weight\n",
      "encoder.blocks.9.mlp.0.bias\n",
      "encoder.blocks.9.mlp.2.weight\n",
      "encoder.blocks.9.mlp.2.bias\n",
      "encoder.blocks.9.mlp_ln.weight\n",
      "encoder.blocks.9.mlp_ln.bias\n",
      "encoder.blocks.10.attn.query.weight\n",
      "encoder.blocks.10.attn.query.bias\n",
      "encoder.blocks.10.attn.key.weight\n",
      "encoder.blocks.10.attn.value.weight\n",
      "encoder.blocks.10.attn.value.bias\n",
      "encoder.blocks.10.attn.out.weight\n",
      "encoder.blocks.10.attn.out.bias\n",
      "encoder.blocks.10.attn_ln.weight\n",
      "encoder.blocks.10.attn_ln.bias\n",
      "encoder.blocks.10.mlp.0.weight\n",
      "encoder.blocks.10.mlp.0.bias\n",
      "encoder.blocks.10.mlp.2.weight\n",
      "encoder.blocks.10.mlp.2.bias\n",
      "encoder.blocks.10.mlp_ln.weight\n",
      "encoder.blocks.10.mlp_ln.bias\n",
      "encoder.blocks.11.attn.query.weight\n",
      "encoder.blocks.11.attn.query.bias\n",
      "encoder.blocks.11.attn.key.weight\n",
      "encoder.blocks.11.attn.value.weight\n",
      "encoder.blocks.11.attn.value.bias\n",
      "encoder.blocks.11.attn.out.weight\n",
      "encoder.blocks.11.attn.out.bias\n",
      "encoder.blocks.11.attn_ln.weight\n",
      "encoder.blocks.11.attn_ln.bias\n",
      "encoder.blocks.11.mlp.0.weight\n",
      "encoder.blocks.11.mlp.0.bias\n",
      "encoder.blocks.11.mlp.2.weight\n",
      "encoder.blocks.11.mlp.2.bias\n",
      "encoder.blocks.11.mlp_ln.weight\n",
      "encoder.blocks.11.mlp_ln.bias\n",
      "encoder.blocks.12.attn.query.weight\n",
      "encoder.blocks.12.attn.query.bias\n",
      "encoder.blocks.12.attn.key.weight\n",
      "encoder.blocks.12.attn.value.weight\n",
      "encoder.blocks.12.attn.value.bias\n",
      "encoder.blocks.12.attn.out.weight\n",
      "encoder.blocks.12.attn.out.bias\n",
      "encoder.blocks.12.attn_ln.weight\n",
      "encoder.blocks.12.attn_ln.bias\n",
      "encoder.blocks.12.mlp.0.weight\n",
      "encoder.blocks.12.mlp.0.bias\n",
      "encoder.blocks.12.mlp.2.weight\n",
      "encoder.blocks.12.mlp.2.bias\n",
      "encoder.blocks.12.mlp_ln.weight\n",
      "encoder.blocks.12.mlp_ln.bias\n",
      "encoder.blocks.13.attn.query.weight\n",
      "encoder.blocks.13.attn.query.bias\n",
      "encoder.blocks.13.attn.key.weight\n",
      "encoder.blocks.13.attn.value.weight\n",
      "encoder.blocks.13.attn.value.bias\n",
      "encoder.blocks.13.attn.out.weight\n",
      "encoder.blocks.13.attn.out.bias\n",
      "encoder.blocks.13.attn_ln.weight\n",
      "encoder.blocks.13.attn_ln.bias\n",
      "encoder.blocks.13.mlp.0.weight\n",
      "encoder.blocks.13.mlp.0.bias\n",
      "encoder.blocks.13.mlp.2.weight\n",
      "encoder.blocks.13.mlp.2.bias\n",
      "encoder.blocks.13.mlp_ln.weight\n",
      "encoder.blocks.13.mlp_ln.bias\n",
      "encoder.blocks.14.attn.query.weight\n",
      "encoder.blocks.14.attn.query.bias\n",
      "encoder.blocks.14.attn.key.weight\n",
      "encoder.blocks.14.attn.value.weight\n",
      "encoder.blocks.14.attn.value.bias\n",
      "encoder.blocks.14.attn.out.weight\n",
      "encoder.blocks.14.attn.out.bias\n",
      "encoder.blocks.14.attn_ln.weight\n",
      "encoder.blocks.14.attn_ln.bias\n",
      "encoder.blocks.14.mlp.0.weight\n",
      "encoder.blocks.14.mlp.0.bias\n",
      "encoder.blocks.14.mlp.2.weight\n",
      "encoder.blocks.14.mlp.2.bias\n",
      "encoder.blocks.14.mlp_ln.weight\n",
      "encoder.blocks.14.mlp_ln.bias\n",
      "encoder.blocks.15.attn.query.weight\n",
      "encoder.blocks.15.attn.query.bias\n",
      "encoder.blocks.15.attn.key.weight\n",
      "encoder.blocks.15.attn.value.weight\n",
      "encoder.blocks.15.attn.value.bias\n",
      "encoder.blocks.15.attn.out.weight\n",
      "encoder.blocks.15.attn.out.bias\n",
      "encoder.blocks.15.attn_ln.weight\n",
      "encoder.blocks.15.attn_ln.bias\n",
      "encoder.blocks.15.mlp.0.weight\n",
      "encoder.blocks.15.mlp.0.bias\n",
      "encoder.blocks.15.mlp.2.weight\n",
      "encoder.blocks.15.mlp.2.bias\n",
      "encoder.blocks.15.mlp_ln.weight\n",
      "encoder.blocks.15.mlp_ln.bias\n",
      "encoder.blocks.16.attn.query.weight\n",
      "encoder.blocks.16.attn.query.bias\n",
      "encoder.blocks.16.attn.key.weight\n",
      "encoder.blocks.16.attn.value.weight\n",
      "encoder.blocks.16.attn.value.bias\n",
      "encoder.blocks.16.attn.out.weight\n",
      "encoder.blocks.16.attn.out.bias\n",
      "encoder.blocks.16.attn_ln.weight\n",
      "encoder.blocks.16.attn_ln.bias\n",
      "encoder.blocks.16.mlp.0.weight\n",
      "encoder.blocks.16.mlp.0.bias\n",
      "encoder.blocks.16.mlp.2.weight\n",
      "encoder.blocks.16.mlp.2.bias\n",
      "encoder.blocks.16.mlp_ln.weight\n",
      "encoder.blocks.16.mlp_ln.bias\n",
      "encoder.blocks.17.attn.query.weight\n",
      "encoder.blocks.17.attn.query.bias\n",
      "encoder.blocks.17.attn.key.weight\n",
      "encoder.blocks.17.attn.value.weight\n",
      "encoder.blocks.17.attn.value.bias\n",
      "encoder.blocks.17.attn.out.weight\n",
      "encoder.blocks.17.attn.out.bias\n",
      "encoder.blocks.17.attn_ln.weight\n",
      "encoder.blocks.17.attn_ln.bias\n",
      "encoder.blocks.17.mlp.0.weight\n",
      "encoder.blocks.17.mlp.0.bias\n",
      "encoder.blocks.17.mlp.2.weight\n",
      "encoder.blocks.17.mlp.2.bias\n",
      "encoder.blocks.17.mlp_ln.weight\n",
      "encoder.blocks.17.mlp_ln.bias\n",
      "encoder.blocks.18.attn.query.weight\n",
      "encoder.blocks.18.attn.query.bias\n",
      "encoder.blocks.18.attn.key.weight\n",
      "encoder.blocks.18.attn.value.weight\n",
      "encoder.blocks.18.attn.value.bias\n",
      "encoder.blocks.18.attn.out.weight\n",
      "encoder.blocks.18.attn.out.bias\n",
      "encoder.blocks.18.attn_ln.weight\n",
      "encoder.blocks.18.attn_ln.bias\n",
      "encoder.blocks.18.mlp.0.weight\n",
      "encoder.blocks.18.mlp.0.bias\n",
      "encoder.blocks.18.mlp.2.weight\n",
      "encoder.blocks.18.mlp.2.bias\n",
      "encoder.blocks.18.mlp_ln.weight\n",
      "encoder.blocks.18.mlp_ln.bias\n",
      "encoder.blocks.19.attn.query.weight\n",
      "encoder.blocks.19.attn.query.bias\n",
      "encoder.blocks.19.attn.key.weight\n",
      "encoder.blocks.19.attn.value.weight\n",
      "encoder.blocks.19.attn.value.bias\n",
      "encoder.blocks.19.attn.out.weight\n",
      "encoder.blocks.19.attn.out.bias\n",
      "encoder.blocks.19.attn_ln.weight\n",
      "encoder.blocks.19.attn_ln.bias\n",
      "encoder.blocks.19.mlp.0.weight\n",
      "encoder.blocks.19.mlp.0.bias\n",
      "encoder.blocks.19.mlp.2.weight\n",
      "encoder.blocks.19.mlp.2.bias\n",
      "encoder.blocks.19.mlp_ln.weight\n",
      "encoder.blocks.19.mlp_ln.bias\n",
      "encoder.blocks.20.attn.query.weight\n",
      "encoder.blocks.20.attn.query.bias\n",
      "encoder.blocks.20.attn.key.weight\n",
      "encoder.blocks.20.attn.value.weight\n",
      "encoder.blocks.20.attn.value.bias\n",
      "encoder.blocks.20.attn.out.weight\n",
      "encoder.blocks.20.attn.out.bias\n",
      "encoder.blocks.20.attn_ln.weight\n",
      "encoder.blocks.20.attn_ln.bias\n",
      "encoder.blocks.20.mlp.0.weight\n",
      "encoder.blocks.20.mlp.0.bias\n",
      "encoder.blocks.20.mlp.2.weight\n",
      "encoder.blocks.20.mlp.2.bias\n",
      "encoder.blocks.20.mlp_ln.weight\n",
      "encoder.blocks.20.mlp_ln.bias\n",
      "encoder.blocks.21.attn.query.weight\n",
      "encoder.blocks.21.attn.query.bias\n",
      "encoder.blocks.21.attn.key.weight\n",
      "encoder.blocks.21.attn.value.weight\n",
      "encoder.blocks.21.attn.value.bias\n",
      "encoder.blocks.21.attn.out.weight\n",
      "encoder.blocks.21.attn.out.bias\n",
      "encoder.blocks.21.attn_ln.weight\n",
      "encoder.blocks.21.attn_ln.bias\n",
      "encoder.blocks.21.mlp.0.weight\n",
      "encoder.blocks.21.mlp.0.bias\n",
      "encoder.blocks.21.mlp.2.weight\n",
      "encoder.blocks.21.mlp.2.bias\n",
      "encoder.blocks.21.mlp_ln.weight\n",
      "encoder.blocks.21.mlp_ln.bias\n",
      "encoder.blocks.22.attn.query.weight\n",
      "encoder.blocks.22.attn.query.bias\n",
      "encoder.blocks.22.attn.key.weight\n",
      "encoder.blocks.22.attn.value.weight\n",
      "encoder.blocks.22.attn.value.bias\n",
      "encoder.blocks.22.attn.out.weight\n",
      "encoder.blocks.22.attn.out.bias\n",
      "encoder.blocks.22.attn_ln.weight\n",
      "encoder.blocks.22.attn_ln.bias\n",
      "encoder.blocks.22.mlp.0.weight\n",
      "encoder.blocks.22.mlp.0.bias\n",
      "encoder.blocks.22.mlp.2.weight\n",
      "encoder.blocks.22.mlp.2.bias\n",
      "encoder.blocks.22.mlp_ln.weight\n",
      "encoder.blocks.22.mlp_ln.bias\n",
      "encoder.blocks.23.attn.query.weight\n",
      "encoder.blocks.23.attn.query.bias\n",
      "encoder.blocks.23.attn.key.weight\n",
      "encoder.blocks.23.attn.value.weight\n",
      "encoder.blocks.23.attn.value.bias\n",
      "encoder.blocks.23.attn.out.weight\n",
      "encoder.blocks.23.attn.out.bias\n",
      "encoder.blocks.23.attn_ln.weight\n",
      "encoder.blocks.23.attn_ln.bias\n",
      "encoder.blocks.23.mlp.0.weight\n",
      "encoder.blocks.23.mlp.0.bias\n",
      "encoder.blocks.23.mlp.2.weight\n",
      "encoder.blocks.23.mlp.2.bias\n",
      "encoder.blocks.23.mlp_ln.weight\n",
      "encoder.blocks.23.mlp_ln.bias\n",
      "encoder.ln_post.weight\n",
      "encoder.ln_post.bias\n",
      "decoder.positional_embedding\n",
      "decoder.token_embedding.weight\n",
      "decoder.blocks.0.attn.query.weight\n",
      "decoder.blocks.0.attn.query.bias\n",
      "decoder.blocks.0.attn.key.weight\n",
      "decoder.blocks.0.attn.value.weight\n",
      "decoder.blocks.0.attn.value.bias\n",
      "decoder.blocks.0.attn.out.weight\n",
      "decoder.blocks.0.attn.out.bias\n",
      "decoder.blocks.0.attn_ln.weight\n",
      "decoder.blocks.0.attn_ln.bias\n",
      "decoder.blocks.0.cross_attn.query.weight\n",
      "decoder.blocks.0.cross_attn.query.bias\n",
      "decoder.blocks.0.cross_attn.key.weight\n",
      "decoder.blocks.0.cross_attn.value.weight\n",
      "decoder.blocks.0.cross_attn.value.bias\n",
      "decoder.blocks.0.cross_attn.out.weight\n",
      "decoder.blocks.0.cross_attn.out.bias\n",
      "decoder.blocks.0.cross_attn_ln.weight\n",
      "decoder.blocks.0.cross_attn_ln.bias\n",
      "decoder.blocks.0.mlp.0.weight\n",
      "decoder.blocks.0.mlp.0.bias\n",
      "decoder.blocks.0.mlp.2.weight\n",
      "decoder.blocks.0.mlp.2.bias\n",
      "decoder.blocks.0.mlp_ln.weight\n",
      "decoder.blocks.0.mlp_ln.bias\n",
      "decoder.blocks.1.attn.query.weight\n",
      "decoder.blocks.1.attn.query.bias\n",
      "decoder.blocks.1.attn.key.weight\n",
      "decoder.blocks.1.attn.value.weight\n",
      "decoder.blocks.1.attn.value.bias\n",
      "decoder.blocks.1.attn.out.weight\n",
      "decoder.blocks.1.attn.out.bias\n",
      "decoder.blocks.1.attn_ln.weight\n",
      "decoder.blocks.1.attn_ln.bias\n",
      "decoder.blocks.1.cross_attn.query.weight\n",
      "decoder.blocks.1.cross_attn.query.bias\n",
      "decoder.blocks.1.cross_attn.key.weight\n",
      "decoder.blocks.1.cross_attn.value.weight\n",
      "decoder.blocks.1.cross_attn.value.bias\n",
      "decoder.blocks.1.cross_attn.out.weight\n",
      "decoder.blocks.1.cross_attn.out.bias\n",
      "decoder.blocks.1.cross_attn_ln.weight\n",
      "decoder.blocks.1.cross_attn_ln.bias\n",
      "decoder.blocks.1.mlp.0.weight\n",
      "decoder.blocks.1.mlp.0.bias\n",
      "decoder.blocks.1.mlp.2.weight\n",
      "decoder.blocks.1.mlp.2.bias\n",
      "decoder.blocks.1.mlp_ln.weight\n",
      "decoder.blocks.1.mlp_ln.bias\n",
      "decoder.blocks.2.attn.query.weight\n",
      "decoder.blocks.2.attn.query.bias\n",
      "decoder.blocks.2.attn.key.weight\n",
      "decoder.blocks.2.attn.value.weight\n",
      "decoder.blocks.2.attn.value.bias\n",
      "decoder.blocks.2.attn.out.weight\n",
      "decoder.blocks.2.attn.out.bias\n",
      "decoder.blocks.2.attn_ln.weight\n",
      "decoder.blocks.2.attn_ln.bias\n",
      "decoder.blocks.2.cross_attn.query.weight\n",
      "decoder.blocks.2.cross_attn.query.bias\n",
      "decoder.blocks.2.cross_attn.key.weight\n",
      "decoder.blocks.2.cross_attn.value.weight\n",
      "decoder.blocks.2.cross_attn.value.bias\n",
      "decoder.blocks.2.cross_attn.out.weight\n",
      "decoder.blocks.2.cross_attn.out.bias\n",
      "decoder.blocks.2.cross_attn_ln.weight\n",
      "decoder.blocks.2.cross_attn_ln.bias\n",
      "decoder.blocks.2.mlp.0.weight\n",
      "decoder.blocks.2.mlp.0.bias\n",
      "decoder.blocks.2.mlp.2.weight\n",
      "decoder.blocks.2.mlp.2.bias\n",
      "decoder.blocks.2.mlp_ln.weight\n",
      "decoder.blocks.2.mlp_ln.bias\n",
      "decoder.blocks.3.attn.query.weight\n",
      "decoder.blocks.3.attn.query.bias\n",
      "decoder.blocks.3.attn.key.weight\n",
      "decoder.blocks.3.attn.value.weight\n",
      "decoder.blocks.3.attn.value.bias\n",
      "decoder.blocks.3.attn.out.weight\n",
      "decoder.blocks.3.attn.out.bias\n",
      "decoder.blocks.3.attn_ln.weight\n",
      "decoder.blocks.3.attn_ln.bias\n",
      "decoder.blocks.3.cross_attn.query.weight\n",
      "decoder.blocks.3.cross_attn.query.bias\n",
      "decoder.blocks.3.cross_attn.key.weight\n",
      "decoder.blocks.3.cross_attn.value.weight\n",
      "decoder.blocks.3.cross_attn.value.bias\n",
      "decoder.blocks.3.cross_attn.out.weight\n",
      "decoder.blocks.3.cross_attn.out.bias\n",
      "decoder.blocks.3.cross_attn_ln.weight\n",
      "decoder.blocks.3.cross_attn_ln.bias\n",
      "decoder.blocks.3.mlp.0.weight\n",
      "decoder.blocks.3.mlp.0.bias\n",
      "decoder.blocks.3.mlp.2.weight\n",
      "decoder.blocks.3.mlp.2.bias\n",
      "decoder.blocks.3.mlp_ln.weight\n",
      "decoder.blocks.3.mlp_ln.bias\n",
      "decoder.blocks.4.attn.query.weight\n",
      "decoder.blocks.4.attn.query.bias\n",
      "decoder.blocks.4.attn.key.weight\n",
      "decoder.blocks.4.attn.value.weight\n",
      "decoder.blocks.4.attn.value.bias\n",
      "decoder.blocks.4.attn.out.weight\n",
      "decoder.blocks.4.attn.out.bias\n",
      "decoder.blocks.4.attn_ln.weight\n",
      "decoder.blocks.4.attn_ln.bias\n",
      "decoder.blocks.4.cross_attn.query.weight\n",
      "decoder.blocks.4.cross_attn.query.bias\n",
      "decoder.blocks.4.cross_attn.key.weight\n",
      "decoder.blocks.4.cross_attn.value.weight\n",
      "decoder.blocks.4.cross_attn.value.bias\n",
      "decoder.blocks.4.cross_attn.out.weight\n",
      "decoder.blocks.4.cross_attn.out.bias\n",
      "decoder.blocks.4.cross_attn_ln.weight\n",
      "decoder.blocks.4.cross_attn_ln.bias\n",
      "decoder.blocks.4.mlp.0.weight\n",
      "decoder.blocks.4.mlp.0.bias\n",
      "decoder.blocks.4.mlp.2.weight\n",
      "decoder.blocks.4.mlp.2.bias\n",
      "decoder.blocks.4.mlp_ln.weight\n",
      "decoder.blocks.4.mlp_ln.bias\n",
      "decoder.blocks.5.attn.query.weight\n",
      "decoder.blocks.5.attn.query.bias\n",
      "decoder.blocks.5.attn.key.weight\n",
      "decoder.blocks.5.attn.value.weight\n",
      "decoder.blocks.5.attn.value.bias\n",
      "decoder.blocks.5.attn.out.weight\n",
      "decoder.blocks.5.attn.out.bias\n",
      "decoder.blocks.5.attn_ln.weight\n",
      "decoder.blocks.5.attn_ln.bias\n",
      "decoder.blocks.5.cross_attn.query.weight\n",
      "decoder.blocks.5.cross_attn.query.bias\n",
      "decoder.blocks.5.cross_attn.key.weight\n",
      "decoder.blocks.5.cross_attn.value.weight\n",
      "decoder.blocks.5.cross_attn.value.bias\n",
      "decoder.blocks.5.cross_attn.out.weight\n",
      "decoder.blocks.5.cross_attn.out.bias\n",
      "decoder.blocks.5.cross_attn_ln.weight\n",
      "decoder.blocks.5.cross_attn_ln.bias\n",
      "decoder.blocks.5.mlp.0.weight\n",
      "decoder.blocks.5.mlp.0.bias\n",
      "decoder.blocks.5.mlp.2.weight\n",
      "decoder.blocks.5.mlp.2.bias\n",
      "decoder.blocks.5.mlp_ln.weight\n",
      "decoder.blocks.5.mlp_ln.bias\n",
      "decoder.blocks.6.attn.query.weight\n",
      "decoder.blocks.6.attn.query.bias\n",
      "decoder.blocks.6.attn.key.weight\n",
      "decoder.blocks.6.attn.value.weight\n",
      "decoder.blocks.6.attn.value.bias\n",
      "decoder.blocks.6.attn.out.weight\n",
      "decoder.blocks.6.attn.out.bias\n",
      "decoder.blocks.6.attn_ln.weight\n",
      "decoder.blocks.6.attn_ln.bias\n",
      "decoder.blocks.6.cross_attn.query.weight\n",
      "decoder.blocks.6.cross_attn.query.bias\n",
      "decoder.blocks.6.cross_attn.key.weight\n",
      "decoder.blocks.6.cross_attn.value.weight\n",
      "decoder.blocks.6.cross_attn.value.bias\n",
      "decoder.blocks.6.cross_attn.out.weight\n",
      "decoder.blocks.6.cross_attn.out.bias\n",
      "decoder.blocks.6.cross_attn_ln.weight\n",
      "decoder.blocks.6.cross_attn_ln.bias\n",
      "decoder.blocks.6.mlp.0.weight\n",
      "decoder.blocks.6.mlp.0.bias\n",
      "decoder.blocks.6.mlp.2.weight\n",
      "decoder.blocks.6.mlp.2.bias\n",
      "decoder.blocks.6.mlp_ln.weight\n",
      "decoder.blocks.6.mlp_ln.bias\n",
      "decoder.blocks.7.attn.query.weight\n",
      "decoder.blocks.7.attn.query.bias\n",
      "decoder.blocks.7.attn.key.weight\n",
      "decoder.blocks.7.attn.value.weight\n",
      "decoder.blocks.7.attn.value.bias\n",
      "decoder.blocks.7.attn.out.weight\n",
      "decoder.blocks.7.attn.out.bias\n",
      "decoder.blocks.7.attn_ln.weight\n",
      "decoder.blocks.7.attn_ln.bias\n",
      "decoder.blocks.7.cross_attn.query.weight\n",
      "decoder.blocks.7.cross_attn.query.bias\n",
      "decoder.blocks.7.cross_attn.key.weight\n",
      "decoder.blocks.7.cross_attn.value.weight\n",
      "decoder.blocks.7.cross_attn.value.bias\n",
      "decoder.blocks.7.cross_attn.out.weight\n",
      "decoder.blocks.7.cross_attn.out.bias\n",
      "decoder.blocks.7.cross_attn_ln.weight\n",
      "decoder.blocks.7.cross_attn_ln.bias\n",
      "decoder.blocks.7.mlp.0.weight\n",
      "decoder.blocks.7.mlp.0.bias\n",
      "decoder.blocks.7.mlp.2.weight\n",
      "decoder.blocks.7.mlp.2.bias\n",
      "decoder.blocks.7.mlp_ln.weight\n",
      "decoder.blocks.7.mlp_ln.bias\n",
      "decoder.blocks.8.attn.query.weight\n",
      "decoder.blocks.8.attn.query.bias\n",
      "decoder.blocks.8.attn.key.weight\n",
      "decoder.blocks.8.attn.value.weight\n",
      "decoder.blocks.8.attn.value.bias\n",
      "decoder.blocks.8.attn.out.weight\n",
      "decoder.blocks.8.attn.out.bias\n",
      "decoder.blocks.8.attn_ln.weight\n",
      "decoder.blocks.8.attn_ln.bias\n",
      "decoder.blocks.8.cross_attn.query.weight\n",
      "decoder.blocks.8.cross_attn.query.bias\n",
      "decoder.blocks.8.cross_attn.key.weight\n",
      "decoder.blocks.8.cross_attn.value.weight\n",
      "decoder.blocks.8.cross_attn.value.bias\n",
      "decoder.blocks.8.cross_attn.out.weight\n",
      "decoder.blocks.8.cross_attn.out.bias\n",
      "decoder.blocks.8.cross_attn_ln.weight\n",
      "decoder.blocks.8.cross_attn_ln.bias\n",
      "decoder.blocks.8.mlp.0.weight\n",
      "decoder.blocks.8.mlp.0.bias\n",
      "decoder.blocks.8.mlp.2.weight\n",
      "decoder.blocks.8.mlp.2.bias\n",
      "decoder.blocks.8.mlp_ln.weight\n",
      "decoder.blocks.8.mlp_ln.bias\n",
      "decoder.blocks.9.attn.query.weight\n",
      "decoder.blocks.9.attn.query.bias\n",
      "decoder.blocks.9.attn.key.weight\n",
      "decoder.blocks.9.attn.value.weight\n",
      "decoder.blocks.9.attn.value.bias\n",
      "decoder.blocks.9.attn.out.weight\n",
      "decoder.blocks.9.attn.out.bias\n",
      "decoder.blocks.9.attn_ln.weight\n",
      "decoder.blocks.9.attn_ln.bias\n",
      "decoder.blocks.9.cross_attn.query.weight\n",
      "decoder.blocks.9.cross_attn.query.bias\n",
      "decoder.blocks.9.cross_attn.key.weight\n",
      "decoder.blocks.9.cross_attn.value.weight\n",
      "decoder.blocks.9.cross_attn.value.bias\n",
      "decoder.blocks.9.cross_attn.out.weight\n",
      "decoder.blocks.9.cross_attn.out.bias\n",
      "decoder.blocks.9.cross_attn_ln.weight\n",
      "decoder.blocks.9.cross_attn_ln.bias\n",
      "decoder.blocks.9.mlp.0.weight\n",
      "decoder.blocks.9.mlp.0.bias\n",
      "decoder.blocks.9.mlp.2.weight\n",
      "decoder.blocks.9.mlp.2.bias\n",
      "decoder.blocks.9.mlp_ln.weight\n",
      "decoder.blocks.9.mlp_ln.bias\n",
      "decoder.blocks.10.attn.query.weight\n",
      "decoder.blocks.10.attn.query.bias\n",
      "decoder.blocks.10.attn.key.weight\n",
      "decoder.blocks.10.attn.value.weight\n",
      "decoder.blocks.10.attn.value.bias\n",
      "decoder.blocks.10.attn.out.weight\n",
      "decoder.blocks.10.attn.out.bias\n",
      "decoder.blocks.10.attn_ln.weight\n",
      "decoder.blocks.10.attn_ln.bias\n",
      "decoder.blocks.10.cross_attn.query.weight\n",
      "decoder.blocks.10.cross_attn.query.bias\n",
      "decoder.blocks.10.cross_attn.key.weight\n",
      "decoder.blocks.10.cross_attn.value.weight\n",
      "decoder.blocks.10.cross_attn.value.bias\n",
      "decoder.blocks.10.cross_attn.out.weight\n",
      "decoder.blocks.10.cross_attn.out.bias\n",
      "decoder.blocks.10.cross_attn_ln.weight\n",
      "decoder.blocks.10.cross_attn_ln.bias\n",
      "decoder.blocks.10.mlp.0.weight\n",
      "decoder.blocks.10.mlp.0.bias\n",
      "decoder.blocks.10.mlp.2.weight\n",
      "decoder.blocks.10.mlp.2.bias\n",
      "decoder.blocks.10.mlp_ln.weight\n",
      "decoder.blocks.10.mlp_ln.bias\n",
      "decoder.blocks.11.attn.query.weight\n",
      "decoder.blocks.11.attn.query.bias\n",
      "decoder.blocks.11.attn.key.weight\n",
      "decoder.blocks.11.attn.value.weight\n",
      "decoder.blocks.11.attn.value.bias\n",
      "decoder.blocks.11.attn.out.weight\n",
      "decoder.blocks.11.attn.out.bias\n",
      "decoder.blocks.11.attn_ln.weight\n",
      "decoder.blocks.11.attn_ln.bias\n",
      "decoder.blocks.11.cross_attn.query.weight\n",
      "decoder.blocks.11.cross_attn.query.bias\n",
      "decoder.blocks.11.cross_attn.key.weight\n",
      "decoder.blocks.11.cross_attn.value.weight\n",
      "decoder.blocks.11.cross_attn.value.bias\n",
      "decoder.blocks.11.cross_attn.out.weight\n",
      "decoder.blocks.11.cross_attn.out.bias\n",
      "decoder.blocks.11.cross_attn_ln.weight\n",
      "decoder.blocks.11.cross_attn_ln.bias\n",
      "decoder.blocks.11.mlp.0.weight\n",
      "decoder.blocks.11.mlp.0.bias\n",
      "decoder.blocks.11.mlp.2.weight\n",
      "decoder.blocks.11.mlp.2.bias\n",
      "decoder.blocks.11.mlp_ln.weight\n",
      "decoder.blocks.11.mlp_ln.bias\n",
      "decoder.blocks.12.attn.query.weight\n",
      "decoder.blocks.12.attn.query.bias\n",
      "decoder.blocks.12.attn.key.weight\n",
      "decoder.blocks.12.attn.value.weight\n",
      "decoder.blocks.12.attn.value.bias\n",
      "decoder.blocks.12.attn.out.weight\n",
      "decoder.blocks.12.attn.out.bias\n",
      "decoder.blocks.12.attn_ln.weight\n",
      "decoder.blocks.12.attn_ln.bias\n",
      "decoder.blocks.12.cross_attn.query.weight\n",
      "decoder.blocks.12.cross_attn.query.bias\n",
      "decoder.blocks.12.cross_attn.key.weight\n",
      "decoder.blocks.12.cross_attn.value.weight\n",
      "decoder.blocks.12.cross_attn.value.bias\n",
      "decoder.blocks.12.cross_attn.out.weight\n",
      "decoder.blocks.12.cross_attn.out.bias\n",
      "decoder.blocks.12.cross_attn_ln.weight\n",
      "decoder.blocks.12.cross_attn_ln.bias\n",
      "decoder.blocks.12.mlp.0.weight\n",
      "decoder.blocks.12.mlp.0.bias\n",
      "decoder.blocks.12.mlp.2.weight\n",
      "decoder.blocks.12.mlp.2.bias\n",
      "decoder.blocks.12.mlp_ln.weight\n",
      "decoder.blocks.12.mlp_ln.bias\n",
      "decoder.blocks.13.attn.query.weight\n",
      "decoder.blocks.13.attn.query.bias\n",
      "decoder.blocks.13.attn.key.weight\n",
      "decoder.blocks.13.attn.value.weight\n",
      "decoder.blocks.13.attn.value.bias\n",
      "decoder.blocks.13.attn.out.weight\n",
      "decoder.blocks.13.attn.out.bias\n",
      "decoder.blocks.13.attn_ln.weight\n",
      "decoder.blocks.13.attn_ln.bias\n",
      "decoder.blocks.13.cross_attn.query.weight\n",
      "decoder.blocks.13.cross_attn.query.bias\n",
      "decoder.blocks.13.cross_attn.key.weight\n",
      "decoder.blocks.13.cross_attn.value.weight\n",
      "decoder.blocks.13.cross_attn.value.bias\n",
      "decoder.blocks.13.cross_attn.out.weight\n",
      "decoder.blocks.13.cross_attn.out.bias\n",
      "decoder.blocks.13.cross_attn_ln.weight\n",
      "decoder.blocks.13.cross_attn_ln.bias\n",
      "decoder.blocks.13.mlp.0.weight\n",
      "decoder.blocks.13.mlp.0.bias\n",
      "decoder.blocks.13.mlp.2.weight\n",
      "decoder.blocks.13.mlp.2.bias\n",
      "decoder.blocks.13.mlp_ln.weight\n",
      "decoder.blocks.13.mlp_ln.bias\n",
      "decoder.blocks.14.attn.query.weight\n",
      "decoder.blocks.14.attn.query.bias\n",
      "decoder.blocks.14.attn.key.weight\n",
      "decoder.blocks.14.attn.value.weight\n",
      "decoder.blocks.14.attn.value.bias\n",
      "decoder.blocks.14.attn.out.weight\n",
      "decoder.blocks.14.attn.out.bias\n",
      "decoder.blocks.14.attn_ln.weight\n",
      "decoder.blocks.14.attn_ln.bias\n",
      "decoder.blocks.14.cross_attn.query.weight\n",
      "decoder.blocks.14.cross_attn.query.bias\n",
      "decoder.blocks.14.cross_attn.key.weight\n",
      "decoder.blocks.14.cross_attn.value.weight\n",
      "decoder.blocks.14.cross_attn.value.bias\n",
      "decoder.blocks.14.cross_attn.out.weight\n",
      "decoder.blocks.14.cross_attn.out.bias\n",
      "decoder.blocks.14.cross_attn_ln.weight\n",
      "decoder.blocks.14.cross_attn_ln.bias\n",
      "decoder.blocks.14.mlp.0.weight\n",
      "decoder.blocks.14.mlp.0.bias\n",
      "decoder.blocks.14.mlp.2.weight\n",
      "decoder.blocks.14.mlp.2.bias\n",
      "decoder.blocks.14.mlp_ln.weight\n",
      "decoder.blocks.14.mlp_ln.bias\n",
      "decoder.blocks.15.attn.query.weight\n",
      "decoder.blocks.15.attn.query.bias\n",
      "decoder.blocks.15.attn.key.weight\n",
      "decoder.blocks.15.attn.value.weight\n",
      "decoder.blocks.15.attn.value.bias\n",
      "decoder.blocks.15.attn.out.weight\n",
      "decoder.blocks.15.attn.out.bias\n",
      "decoder.blocks.15.attn_ln.weight\n",
      "decoder.blocks.15.attn_ln.bias\n",
      "decoder.blocks.15.cross_attn.query.weight\n",
      "decoder.blocks.15.cross_attn.query.bias\n",
      "decoder.blocks.15.cross_attn.key.weight\n",
      "decoder.blocks.15.cross_attn.value.weight\n",
      "decoder.blocks.15.cross_attn.value.bias\n",
      "decoder.blocks.15.cross_attn.out.weight\n",
      "decoder.blocks.15.cross_attn.out.bias\n",
      "decoder.blocks.15.cross_attn_ln.weight\n",
      "decoder.blocks.15.cross_attn_ln.bias\n",
      "decoder.blocks.15.mlp.0.weight\n",
      "decoder.blocks.15.mlp.0.bias\n",
      "decoder.blocks.15.mlp.2.weight\n",
      "decoder.blocks.15.mlp.2.bias\n",
      "decoder.blocks.15.mlp_ln.weight\n",
      "decoder.blocks.15.mlp_ln.bias\n",
      "decoder.blocks.16.attn.query.weight\n",
      "decoder.blocks.16.attn.query.bias\n",
      "decoder.blocks.16.attn.key.weight\n",
      "decoder.blocks.16.attn.value.weight\n",
      "decoder.blocks.16.attn.value.bias\n",
      "decoder.blocks.16.attn.out.weight\n",
      "decoder.blocks.16.attn.out.bias\n",
      "decoder.blocks.16.attn_ln.weight\n",
      "decoder.blocks.16.attn_ln.bias\n",
      "decoder.blocks.16.cross_attn.query.weight\n",
      "decoder.blocks.16.cross_attn.query.bias\n",
      "decoder.blocks.16.cross_attn.key.weight\n",
      "decoder.blocks.16.cross_attn.value.weight\n",
      "decoder.blocks.16.cross_attn.value.bias\n",
      "decoder.blocks.16.cross_attn.out.weight\n",
      "decoder.blocks.16.cross_attn.out.bias\n",
      "decoder.blocks.16.cross_attn_ln.weight\n",
      "decoder.blocks.16.cross_attn_ln.bias\n",
      "decoder.blocks.16.mlp.0.weight\n",
      "decoder.blocks.16.mlp.0.bias\n",
      "decoder.blocks.16.mlp.2.weight\n",
      "decoder.blocks.16.mlp.2.bias\n",
      "decoder.blocks.16.mlp_ln.weight\n",
      "decoder.blocks.16.mlp_ln.bias\n",
      "decoder.blocks.17.attn.query.weight\n",
      "decoder.blocks.17.attn.query.bias\n",
      "decoder.blocks.17.attn.key.weight\n",
      "decoder.blocks.17.attn.value.weight\n",
      "decoder.blocks.17.attn.value.bias\n",
      "decoder.blocks.17.attn.out.weight\n",
      "decoder.blocks.17.attn.out.bias\n",
      "decoder.blocks.17.attn_ln.weight\n",
      "decoder.blocks.17.attn_ln.bias\n",
      "decoder.blocks.17.cross_attn.query.weight\n",
      "decoder.blocks.17.cross_attn.query.bias\n",
      "decoder.blocks.17.cross_attn.key.weight\n",
      "decoder.blocks.17.cross_attn.value.weight\n",
      "decoder.blocks.17.cross_attn.value.bias\n",
      "decoder.blocks.17.cross_attn.out.weight\n",
      "decoder.blocks.17.cross_attn.out.bias\n",
      "decoder.blocks.17.cross_attn_ln.weight\n",
      "decoder.blocks.17.cross_attn_ln.bias\n",
      "decoder.blocks.17.mlp.0.weight\n",
      "decoder.blocks.17.mlp.0.bias\n",
      "decoder.blocks.17.mlp.2.weight\n",
      "decoder.blocks.17.mlp.2.bias\n",
      "decoder.blocks.17.mlp_ln.weight\n",
      "decoder.blocks.17.mlp_ln.bias\n",
      "decoder.blocks.18.attn.query.weight\n",
      "decoder.blocks.18.attn.query.bias\n",
      "decoder.blocks.18.attn.key.weight\n",
      "decoder.blocks.18.attn.value.weight\n",
      "decoder.blocks.18.attn.value.bias\n",
      "decoder.blocks.18.attn.out.weight\n",
      "decoder.blocks.18.attn.out.bias\n",
      "decoder.blocks.18.attn_ln.weight\n",
      "decoder.blocks.18.attn_ln.bias\n",
      "decoder.blocks.18.cross_attn.query.weight\n",
      "decoder.blocks.18.cross_attn.query.bias\n",
      "decoder.blocks.18.cross_attn.key.weight\n",
      "decoder.blocks.18.cross_attn.value.weight\n",
      "decoder.blocks.18.cross_attn.value.bias\n",
      "decoder.blocks.18.cross_attn.out.weight\n",
      "decoder.blocks.18.cross_attn.out.bias\n",
      "decoder.blocks.18.cross_attn_ln.weight\n",
      "decoder.blocks.18.cross_attn_ln.bias\n",
      "decoder.blocks.18.mlp.0.weight\n",
      "decoder.blocks.18.mlp.0.bias\n",
      "decoder.blocks.18.mlp.2.weight\n",
      "decoder.blocks.18.mlp.2.bias\n",
      "decoder.blocks.18.mlp_ln.weight\n",
      "decoder.blocks.18.mlp_ln.bias\n",
      "decoder.blocks.19.attn.query.weight\n",
      "decoder.blocks.19.attn.query.bias\n",
      "decoder.blocks.19.attn.key.weight\n",
      "decoder.blocks.19.attn.value.weight\n",
      "decoder.blocks.19.attn.value.bias\n",
      "decoder.blocks.19.attn.out.weight\n",
      "decoder.blocks.19.attn.out.bias\n",
      "decoder.blocks.19.attn_ln.weight\n",
      "decoder.blocks.19.attn_ln.bias\n",
      "decoder.blocks.19.cross_attn.query.weight\n",
      "decoder.blocks.19.cross_attn.query.bias\n",
      "decoder.blocks.19.cross_attn.key.weight\n",
      "decoder.blocks.19.cross_attn.value.weight\n",
      "decoder.blocks.19.cross_attn.value.bias\n",
      "decoder.blocks.19.cross_attn.out.weight\n",
      "decoder.blocks.19.cross_attn.out.bias\n",
      "decoder.blocks.19.cross_attn_ln.weight\n",
      "decoder.blocks.19.cross_attn_ln.bias\n",
      "decoder.blocks.19.mlp.0.weight\n",
      "decoder.blocks.19.mlp.0.bias\n",
      "decoder.blocks.19.mlp.2.weight\n",
      "decoder.blocks.19.mlp.2.bias\n",
      "decoder.blocks.19.mlp_ln.weight\n",
      "decoder.blocks.19.mlp_ln.bias\n",
      "decoder.blocks.20.attn.query.weight\n",
      "decoder.blocks.20.attn.query.bias\n",
      "decoder.blocks.20.attn.key.weight\n",
      "decoder.blocks.20.attn.value.weight\n",
      "decoder.blocks.20.attn.value.bias\n",
      "decoder.blocks.20.attn.out.weight\n",
      "decoder.blocks.20.attn.out.bias\n",
      "decoder.blocks.20.attn_ln.weight\n",
      "decoder.blocks.20.attn_ln.bias\n",
      "decoder.blocks.20.cross_attn.query.weight\n",
      "decoder.blocks.20.cross_attn.query.bias\n",
      "decoder.blocks.20.cross_attn.key.weight\n",
      "decoder.blocks.20.cross_attn.value.weight\n",
      "decoder.blocks.20.cross_attn.value.bias\n",
      "decoder.blocks.20.cross_attn.out.weight\n",
      "decoder.blocks.20.cross_attn.out.bias\n",
      "decoder.blocks.20.cross_attn_ln.weight\n",
      "decoder.blocks.20.cross_attn_ln.bias\n",
      "decoder.blocks.20.mlp.0.weight\n",
      "decoder.blocks.20.mlp.0.bias\n",
      "decoder.blocks.20.mlp.2.weight\n",
      "decoder.blocks.20.mlp.2.bias\n",
      "decoder.blocks.20.mlp_ln.weight\n",
      "decoder.blocks.20.mlp_ln.bias\n",
      "decoder.blocks.21.attn.query.weight\n",
      "decoder.blocks.21.attn.query.bias\n",
      "decoder.blocks.21.attn.key.weight\n",
      "decoder.blocks.21.attn.value.weight\n",
      "decoder.blocks.21.attn.value.bias\n",
      "decoder.blocks.21.attn.out.weight\n",
      "decoder.blocks.21.attn.out.bias\n",
      "decoder.blocks.21.attn_ln.weight\n",
      "decoder.blocks.21.attn_ln.bias\n",
      "decoder.blocks.21.cross_attn.query.weight\n",
      "decoder.blocks.21.cross_attn.query.bias\n",
      "decoder.blocks.21.cross_attn.key.weight\n",
      "decoder.blocks.21.cross_attn.value.weight\n",
      "decoder.blocks.21.cross_attn.value.bias\n",
      "decoder.blocks.21.cross_attn.out.weight\n",
      "decoder.blocks.21.cross_attn.out.bias\n",
      "decoder.blocks.21.cross_attn_ln.weight\n",
      "decoder.blocks.21.cross_attn_ln.bias\n",
      "decoder.blocks.21.mlp.0.weight\n",
      "decoder.blocks.21.mlp.0.bias\n",
      "decoder.blocks.21.mlp.2.weight\n",
      "decoder.blocks.21.mlp.2.bias\n",
      "decoder.blocks.21.mlp_ln.weight\n",
      "decoder.blocks.21.mlp_ln.bias\n",
      "decoder.blocks.22.attn.query.weight\n",
      "decoder.blocks.22.attn.query.bias\n",
      "decoder.blocks.22.attn.key.weight\n",
      "decoder.blocks.22.attn.value.weight\n",
      "decoder.blocks.22.attn.value.bias\n",
      "decoder.blocks.22.attn.out.weight\n",
      "decoder.blocks.22.attn.out.bias\n",
      "decoder.blocks.22.attn_ln.weight\n",
      "decoder.blocks.22.attn_ln.bias\n",
      "decoder.blocks.22.cross_attn.query.weight\n",
      "decoder.blocks.22.cross_attn.query.bias\n",
      "decoder.blocks.22.cross_attn.key.weight\n",
      "decoder.blocks.22.cross_attn.value.weight\n",
      "decoder.blocks.22.cross_attn.value.bias\n",
      "decoder.blocks.22.cross_attn.out.weight\n",
      "decoder.blocks.22.cross_attn.out.bias\n",
      "decoder.blocks.22.cross_attn_ln.weight\n",
      "decoder.blocks.22.cross_attn_ln.bias\n",
      "decoder.blocks.22.mlp.0.weight\n",
      "decoder.blocks.22.mlp.0.bias\n",
      "decoder.blocks.22.mlp.2.weight\n",
      "decoder.blocks.22.mlp.2.bias\n",
      "decoder.blocks.22.mlp_ln.weight\n",
      "decoder.blocks.22.mlp_ln.bias\n",
      "decoder.blocks.23.attn.query.weight\n",
      "decoder.blocks.23.attn.query.bias\n",
      "decoder.blocks.23.attn.key.weight\n",
      "decoder.blocks.23.attn.value.weight\n",
      "decoder.blocks.23.attn.value.bias\n",
      "decoder.blocks.23.attn.out.weight\n",
      "decoder.blocks.23.attn.out.bias\n",
      "decoder.blocks.23.attn_ln.weight\n",
      "decoder.blocks.23.attn_ln.bias\n",
      "decoder.blocks.23.cross_attn.query.weight\n",
      "decoder.blocks.23.cross_attn.query.bias\n",
      "decoder.blocks.23.cross_attn.key.weight\n",
      "decoder.blocks.23.cross_attn.value.weight\n",
      "decoder.blocks.23.cross_attn.value.bias\n",
      "decoder.blocks.23.cross_attn.out.weight\n",
      "decoder.blocks.23.cross_attn.out.bias\n",
      "decoder.blocks.23.cross_attn_ln.weight\n",
      "decoder.blocks.23.cross_attn_ln.bias\n",
      "decoder.blocks.23.mlp.0.weight\n",
      "decoder.blocks.23.mlp.0.bias\n",
      "decoder.blocks.23.mlp.2.weight\n",
      "decoder.blocks.23.mlp.2.bias\n",
      "decoder.blocks.23.mlp_ln.weight\n",
      "decoder.blocks.23.mlp_ln.bias\n",
      "decoder.ln.weight\n",
      "decoder.ln.bias\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.encoder.conv1.weight\n",
      "model.encoder.conv1.bias\n",
      "model.encoder.conv2.weight\n",
      "model.encoder.conv2.bias\n",
      "model.encoder.embed_positions.weight\n",
      "model.encoder.layers.0.self_attn.k_proj.weight\n",
      "model.encoder.layers.0.self_attn.v_proj.weight\n",
      "model.encoder.layers.0.self_attn.v_proj.bias\n",
      "model.encoder.layers.0.self_attn.q_proj.weight\n",
      "model.encoder.layers.0.self_attn.q_proj.bias\n",
      "model.encoder.layers.0.self_attn.out_proj.weight\n",
      "model.encoder.layers.0.self_attn.out_proj.bias\n",
      "model.encoder.layers.0.self_attn_layer_norm.weight\n",
      "model.encoder.layers.0.self_attn_layer_norm.bias\n",
      "model.encoder.layers.0.fc1.weight\n",
      "model.encoder.layers.0.fc1.bias\n",
      "model.encoder.layers.0.fc2.weight\n",
      "model.encoder.layers.0.fc2.bias\n",
      "model.encoder.layers.0.final_layer_norm.weight\n",
      "model.encoder.layers.0.final_layer_norm.bias\n",
      "model.encoder.layers.1.self_attn.k_proj.weight\n",
      "model.encoder.layers.1.self_attn.v_proj.weight\n",
      "model.encoder.layers.1.self_attn.v_proj.bias\n",
      "model.encoder.layers.1.self_attn.q_proj.weight\n",
      "model.encoder.layers.1.self_attn.q_proj.bias\n",
      "model.encoder.layers.1.self_attn.out_proj.weight\n",
      "model.encoder.layers.1.self_attn.out_proj.bias\n",
      "model.encoder.layers.1.self_attn_layer_norm.weight\n",
      "model.encoder.layers.1.self_attn_layer_norm.bias\n",
      "model.encoder.layers.1.fc1.weight\n",
      "model.encoder.layers.1.fc1.bias\n",
      "model.encoder.layers.1.fc2.weight\n",
      "model.encoder.layers.1.fc2.bias\n",
      "model.encoder.layers.1.final_layer_norm.weight\n",
      "model.encoder.layers.1.final_layer_norm.bias\n",
      "model.encoder.layers.2.self_attn.k_proj.weight\n",
      "model.encoder.layers.2.self_attn.v_proj.weight\n",
      "model.encoder.layers.2.self_attn.v_proj.bias\n",
      "model.encoder.layers.2.self_attn.q_proj.weight\n",
      "model.encoder.layers.2.self_attn.q_proj.bias\n",
      "model.encoder.layers.2.self_attn.out_proj.weight\n",
      "model.encoder.layers.2.self_attn.out_proj.bias\n",
      "model.encoder.layers.2.self_attn_layer_norm.weight\n",
      "model.encoder.layers.2.self_attn_layer_norm.bias\n",
      "model.encoder.layers.2.fc1.weight\n",
      "model.encoder.layers.2.fc1.bias\n",
      "model.encoder.layers.2.fc2.weight\n",
      "model.encoder.layers.2.fc2.bias\n",
      "model.encoder.layers.2.final_layer_norm.weight\n",
      "model.encoder.layers.2.final_layer_norm.bias\n",
      "model.encoder.layers.3.self_attn.k_proj.weight\n",
      "model.encoder.layers.3.self_attn.v_proj.weight\n",
      "model.encoder.layers.3.self_attn.v_proj.bias\n",
      "model.encoder.layers.3.self_attn.q_proj.weight\n",
      "model.encoder.layers.3.self_attn.q_proj.bias\n",
      "model.encoder.layers.3.self_attn.out_proj.weight\n",
      "model.encoder.layers.3.self_attn.out_proj.bias\n",
      "model.encoder.layers.3.self_attn_layer_norm.weight\n",
      "model.encoder.layers.3.self_attn_layer_norm.bias\n",
      "model.encoder.layers.3.fc1.weight\n",
      "model.encoder.layers.3.fc1.bias\n",
      "model.encoder.layers.3.fc2.weight\n",
      "model.encoder.layers.3.fc2.bias\n",
      "model.encoder.layers.3.final_layer_norm.weight\n",
      "model.encoder.layers.3.final_layer_norm.bias\n",
      "model.encoder.layers.4.self_attn.k_proj.weight\n",
      "model.encoder.layers.4.self_attn.v_proj.weight\n",
      "model.encoder.layers.4.self_attn.v_proj.bias\n",
      "model.encoder.layers.4.self_attn.q_proj.weight\n",
      "model.encoder.layers.4.self_attn.q_proj.bias\n",
      "model.encoder.layers.4.self_attn.out_proj.weight\n",
      "model.encoder.layers.4.self_attn.out_proj.bias\n",
      "model.encoder.layers.4.self_attn_layer_norm.weight\n",
      "model.encoder.layers.4.self_attn_layer_norm.bias\n",
      "model.encoder.layers.4.fc1.weight\n",
      "model.encoder.layers.4.fc1.bias\n",
      "model.encoder.layers.4.fc2.weight\n",
      "model.encoder.layers.4.fc2.bias\n",
      "model.encoder.layers.4.final_layer_norm.weight\n",
      "model.encoder.layers.4.final_layer_norm.bias\n",
      "model.encoder.layers.5.self_attn.k_proj.weight\n",
      "model.encoder.layers.5.self_attn.v_proj.weight\n",
      "model.encoder.layers.5.self_attn.v_proj.bias\n",
      "model.encoder.layers.5.self_attn.q_proj.weight\n",
      "model.encoder.layers.5.self_attn.q_proj.bias\n",
      "model.encoder.layers.5.self_attn.out_proj.weight\n",
      "model.encoder.layers.5.self_attn.out_proj.bias\n",
      "model.encoder.layers.5.self_attn_layer_norm.weight\n",
      "model.encoder.layers.5.self_attn_layer_norm.bias\n",
      "model.encoder.layers.5.fc1.weight\n",
      "model.encoder.layers.5.fc1.bias\n",
      "model.encoder.layers.5.fc2.weight\n",
      "model.encoder.layers.5.fc2.bias\n",
      "model.encoder.layers.5.final_layer_norm.weight\n",
      "model.encoder.layers.5.final_layer_norm.bias\n",
      "model.encoder.layers.6.self_attn.k_proj.weight\n",
      "model.encoder.layers.6.self_attn.v_proj.weight\n",
      "model.encoder.layers.6.self_attn.v_proj.bias\n",
      "model.encoder.layers.6.self_attn.q_proj.weight\n",
      "model.encoder.layers.6.self_attn.q_proj.bias\n",
      "model.encoder.layers.6.self_attn.out_proj.weight\n",
      "model.encoder.layers.6.self_attn.out_proj.bias\n",
      "model.encoder.layers.6.self_attn_layer_norm.weight\n",
      "model.encoder.layers.6.self_attn_layer_norm.bias\n",
      "model.encoder.layers.6.fc1.weight\n",
      "model.encoder.layers.6.fc1.bias\n",
      "model.encoder.layers.6.fc2.weight\n",
      "model.encoder.layers.6.fc2.bias\n",
      "model.encoder.layers.6.final_layer_norm.weight\n",
      "model.encoder.layers.6.final_layer_norm.bias\n",
      "model.encoder.layers.7.self_attn.k_proj.weight\n",
      "model.encoder.layers.7.self_attn.v_proj.weight\n",
      "model.encoder.layers.7.self_attn.v_proj.bias\n",
      "model.encoder.layers.7.self_attn.q_proj.weight\n",
      "model.encoder.layers.7.self_attn.q_proj.bias\n",
      "model.encoder.layers.7.self_attn.out_proj.weight\n",
      "model.encoder.layers.7.self_attn.out_proj.bias\n",
      "model.encoder.layers.7.self_attn_layer_norm.weight\n",
      "model.encoder.layers.7.self_attn_layer_norm.bias\n",
      "model.encoder.layers.7.fc1.weight\n",
      "model.encoder.layers.7.fc1.bias\n",
      "model.encoder.layers.7.fc2.weight\n",
      "model.encoder.layers.7.fc2.bias\n",
      "model.encoder.layers.7.final_layer_norm.weight\n",
      "model.encoder.layers.7.final_layer_norm.bias\n",
      "model.encoder.layers.8.self_attn.k_proj.weight\n",
      "model.encoder.layers.8.self_attn.v_proj.weight\n",
      "model.encoder.layers.8.self_attn.v_proj.bias\n",
      "model.encoder.layers.8.self_attn.q_proj.weight\n",
      "model.encoder.layers.8.self_attn.q_proj.bias\n",
      "model.encoder.layers.8.self_attn.out_proj.weight\n",
      "model.encoder.layers.8.self_attn.out_proj.bias\n",
      "model.encoder.layers.8.self_attn_layer_norm.weight\n",
      "model.encoder.layers.8.self_attn_layer_norm.bias\n",
      "model.encoder.layers.8.fc1.weight\n",
      "model.encoder.layers.8.fc1.bias\n",
      "model.encoder.layers.8.fc2.weight\n",
      "model.encoder.layers.8.fc2.bias\n",
      "model.encoder.layers.8.final_layer_norm.weight\n",
      "model.encoder.layers.8.final_layer_norm.bias\n",
      "model.encoder.layers.9.self_attn.k_proj.weight\n",
      "model.encoder.layers.9.self_attn.v_proj.weight\n",
      "model.encoder.layers.9.self_attn.v_proj.bias\n",
      "model.encoder.layers.9.self_attn.q_proj.weight\n",
      "model.encoder.layers.9.self_attn.q_proj.bias\n",
      "model.encoder.layers.9.self_attn.out_proj.weight\n",
      "model.encoder.layers.9.self_attn.out_proj.bias\n",
      "model.encoder.layers.9.self_attn_layer_norm.weight\n",
      "model.encoder.layers.9.self_attn_layer_norm.bias\n",
      "model.encoder.layers.9.fc1.weight\n",
      "model.encoder.layers.9.fc1.bias\n",
      "model.encoder.layers.9.fc2.weight\n",
      "model.encoder.layers.9.fc2.bias\n",
      "model.encoder.layers.9.final_layer_norm.weight\n",
      "model.encoder.layers.9.final_layer_norm.bias\n",
      "model.encoder.layers.10.self_attn.k_proj.weight\n",
      "model.encoder.layers.10.self_attn.v_proj.weight\n",
      "model.encoder.layers.10.self_attn.v_proj.bias\n",
      "model.encoder.layers.10.self_attn.q_proj.weight\n",
      "model.encoder.layers.10.self_attn.q_proj.bias\n",
      "model.encoder.layers.10.self_attn.out_proj.weight\n",
      "model.encoder.layers.10.self_attn.out_proj.bias\n",
      "model.encoder.layers.10.self_attn_layer_norm.weight\n",
      "model.encoder.layers.10.self_attn_layer_norm.bias\n",
      "model.encoder.layers.10.fc1.weight\n",
      "model.encoder.layers.10.fc1.bias\n",
      "model.encoder.layers.10.fc2.weight\n",
      "model.encoder.layers.10.fc2.bias\n",
      "model.encoder.layers.10.final_layer_norm.weight\n",
      "model.encoder.layers.10.final_layer_norm.bias\n",
      "model.encoder.layers.11.self_attn.k_proj.weight\n",
      "model.encoder.layers.11.self_attn.v_proj.weight\n",
      "model.encoder.layers.11.self_attn.v_proj.bias\n",
      "model.encoder.layers.11.self_attn.q_proj.weight\n",
      "model.encoder.layers.11.self_attn.q_proj.bias\n",
      "model.encoder.layers.11.self_attn.out_proj.weight\n",
      "model.encoder.layers.11.self_attn.out_proj.bias\n",
      "model.encoder.layers.11.self_attn_layer_norm.weight\n",
      "model.encoder.layers.11.self_attn_layer_norm.bias\n",
      "model.encoder.layers.11.fc1.weight\n",
      "model.encoder.layers.11.fc1.bias\n",
      "model.encoder.layers.11.fc2.weight\n",
      "model.encoder.layers.11.fc2.bias\n",
      "model.encoder.layers.11.final_layer_norm.weight\n",
      "model.encoder.layers.11.final_layer_norm.bias\n",
      "model.encoder.layers.12.self_attn.k_proj.weight\n",
      "model.encoder.layers.12.self_attn.v_proj.weight\n",
      "model.encoder.layers.12.self_attn.v_proj.bias\n",
      "model.encoder.layers.12.self_attn.q_proj.weight\n",
      "model.encoder.layers.12.self_attn.q_proj.bias\n",
      "model.encoder.layers.12.self_attn.out_proj.weight\n",
      "model.encoder.layers.12.self_attn.out_proj.bias\n",
      "model.encoder.layers.12.self_attn_layer_norm.weight\n",
      "model.encoder.layers.12.self_attn_layer_norm.bias\n",
      "model.encoder.layers.12.fc1.weight\n",
      "model.encoder.layers.12.fc1.bias\n",
      "model.encoder.layers.12.fc2.weight\n",
      "model.encoder.layers.12.fc2.bias\n",
      "model.encoder.layers.12.final_layer_norm.weight\n",
      "model.encoder.layers.12.final_layer_norm.bias\n",
      "model.encoder.layers.13.self_attn.k_proj.weight\n",
      "model.encoder.layers.13.self_attn.v_proj.weight\n",
      "model.encoder.layers.13.self_attn.v_proj.bias\n",
      "model.encoder.layers.13.self_attn.q_proj.weight\n",
      "model.encoder.layers.13.self_attn.q_proj.bias\n",
      "model.encoder.layers.13.self_attn.out_proj.weight\n",
      "model.encoder.layers.13.self_attn.out_proj.bias\n",
      "model.encoder.layers.13.self_attn_layer_norm.weight\n",
      "model.encoder.layers.13.self_attn_layer_norm.bias\n",
      "model.encoder.layers.13.fc1.weight\n",
      "model.encoder.layers.13.fc1.bias\n",
      "model.encoder.layers.13.fc2.weight\n",
      "model.encoder.layers.13.fc2.bias\n",
      "model.encoder.layers.13.final_layer_norm.weight\n",
      "model.encoder.layers.13.final_layer_norm.bias\n",
      "model.encoder.layers.14.self_attn.k_proj.weight\n",
      "model.encoder.layers.14.self_attn.v_proj.weight\n",
      "model.encoder.layers.14.self_attn.v_proj.bias\n",
      "model.encoder.layers.14.self_attn.q_proj.weight\n",
      "model.encoder.layers.14.self_attn.q_proj.bias\n",
      "model.encoder.layers.14.self_attn.out_proj.weight\n",
      "model.encoder.layers.14.self_attn.out_proj.bias\n",
      "model.encoder.layers.14.self_attn_layer_norm.weight\n",
      "model.encoder.layers.14.self_attn_layer_norm.bias\n",
      "model.encoder.layers.14.fc1.weight\n",
      "model.encoder.layers.14.fc1.bias\n",
      "model.encoder.layers.14.fc2.weight\n",
      "model.encoder.layers.14.fc2.bias\n",
      "model.encoder.layers.14.final_layer_norm.weight\n",
      "model.encoder.layers.14.final_layer_norm.bias\n",
      "model.encoder.layers.15.self_attn.k_proj.weight\n",
      "model.encoder.layers.15.self_attn.v_proj.weight\n",
      "model.encoder.layers.15.self_attn.v_proj.bias\n",
      "model.encoder.layers.15.self_attn.q_proj.weight\n",
      "model.encoder.layers.15.self_attn.q_proj.bias\n",
      "model.encoder.layers.15.self_attn.out_proj.weight\n",
      "model.encoder.layers.15.self_attn.out_proj.bias\n",
      "model.encoder.layers.15.self_attn_layer_norm.weight\n",
      "model.encoder.layers.15.self_attn_layer_norm.bias\n",
      "model.encoder.layers.15.fc1.weight\n",
      "model.encoder.layers.15.fc1.bias\n",
      "model.encoder.layers.15.fc2.weight\n",
      "model.encoder.layers.15.fc2.bias\n",
      "model.encoder.layers.15.final_layer_norm.weight\n",
      "model.encoder.layers.15.final_layer_norm.bias\n",
      "model.encoder.layers.16.self_attn.k_proj.weight\n",
      "model.encoder.layers.16.self_attn.v_proj.weight\n",
      "model.encoder.layers.16.self_attn.v_proj.bias\n",
      "model.encoder.layers.16.self_attn.q_proj.weight\n",
      "model.encoder.layers.16.self_attn.q_proj.bias\n",
      "model.encoder.layers.16.self_attn.out_proj.weight\n",
      "model.encoder.layers.16.self_attn.out_proj.bias\n",
      "model.encoder.layers.16.self_attn_layer_norm.weight\n",
      "model.encoder.layers.16.self_attn_layer_norm.bias\n",
      "model.encoder.layers.16.fc1.weight\n",
      "model.encoder.layers.16.fc1.bias\n",
      "model.encoder.layers.16.fc2.weight\n",
      "model.encoder.layers.16.fc2.bias\n",
      "model.encoder.layers.16.final_layer_norm.weight\n",
      "model.encoder.layers.16.final_layer_norm.bias\n",
      "model.encoder.layers.17.self_attn.k_proj.weight\n",
      "model.encoder.layers.17.self_attn.v_proj.weight\n",
      "model.encoder.layers.17.self_attn.v_proj.bias\n",
      "model.encoder.layers.17.self_attn.q_proj.weight\n",
      "model.encoder.layers.17.self_attn.q_proj.bias\n",
      "model.encoder.layers.17.self_attn.out_proj.weight\n",
      "model.encoder.layers.17.self_attn.out_proj.bias\n",
      "model.encoder.layers.17.self_attn_layer_norm.weight\n",
      "model.encoder.layers.17.self_attn_layer_norm.bias\n",
      "model.encoder.layers.17.fc1.weight\n",
      "model.encoder.layers.17.fc1.bias\n",
      "model.encoder.layers.17.fc2.weight\n",
      "model.encoder.layers.17.fc2.bias\n",
      "model.encoder.layers.17.final_layer_norm.weight\n",
      "model.encoder.layers.17.final_layer_norm.bias\n",
      "model.encoder.layers.18.self_attn.k_proj.weight\n",
      "model.encoder.layers.18.self_attn.v_proj.weight\n",
      "model.encoder.layers.18.self_attn.v_proj.bias\n",
      "model.encoder.layers.18.self_attn.q_proj.weight\n",
      "model.encoder.layers.18.self_attn.q_proj.bias\n",
      "model.encoder.layers.18.self_attn.out_proj.weight\n",
      "model.encoder.layers.18.self_attn.out_proj.bias\n",
      "model.encoder.layers.18.self_attn_layer_norm.weight\n",
      "model.encoder.layers.18.self_attn_layer_norm.bias\n",
      "model.encoder.layers.18.fc1.weight\n",
      "model.encoder.layers.18.fc1.bias\n",
      "model.encoder.layers.18.fc2.weight\n",
      "model.encoder.layers.18.fc2.bias\n",
      "model.encoder.layers.18.final_layer_norm.weight\n",
      "model.encoder.layers.18.final_layer_norm.bias\n",
      "model.encoder.layers.19.self_attn.k_proj.weight\n",
      "model.encoder.layers.19.self_attn.v_proj.weight\n",
      "model.encoder.layers.19.self_attn.v_proj.bias\n",
      "model.encoder.layers.19.self_attn.q_proj.weight\n",
      "model.encoder.layers.19.self_attn.q_proj.bias\n",
      "model.encoder.layers.19.self_attn.out_proj.weight\n",
      "model.encoder.layers.19.self_attn.out_proj.bias\n",
      "model.encoder.layers.19.self_attn_layer_norm.weight\n",
      "model.encoder.layers.19.self_attn_layer_norm.bias\n",
      "model.encoder.layers.19.fc1.weight\n",
      "model.encoder.layers.19.fc1.bias\n",
      "model.encoder.layers.19.fc2.weight\n",
      "model.encoder.layers.19.fc2.bias\n",
      "model.encoder.layers.19.final_layer_norm.weight\n",
      "model.encoder.layers.19.final_layer_norm.bias\n",
      "model.encoder.layers.20.self_attn.k_proj.weight\n",
      "model.encoder.layers.20.self_attn.v_proj.weight\n",
      "model.encoder.layers.20.self_attn.v_proj.bias\n",
      "model.encoder.layers.20.self_attn.q_proj.weight\n",
      "model.encoder.layers.20.self_attn.q_proj.bias\n",
      "model.encoder.layers.20.self_attn.out_proj.weight\n",
      "model.encoder.layers.20.self_attn.out_proj.bias\n",
      "model.encoder.layers.20.self_attn_layer_norm.weight\n",
      "model.encoder.layers.20.self_attn_layer_norm.bias\n",
      "model.encoder.layers.20.fc1.weight\n",
      "model.encoder.layers.20.fc1.bias\n",
      "model.encoder.layers.20.fc2.weight\n",
      "model.encoder.layers.20.fc2.bias\n",
      "model.encoder.layers.20.final_layer_norm.weight\n",
      "model.encoder.layers.20.final_layer_norm.bias\n",
      "model.encoder.layers.21.self_attn.k_proj.weight\n",
      "model.encoder.layers.21.self_attn.v_proj.weight\n",
      "model.encoder.layers.21.self_attn.v_proj.bias\n",
      "model.encoder.layers.21.self_attn.q_proj.weight\n",
      "model.encoder.layers.21.self_attn.q_proj.bias\n",
      "model.encoder.layers.21.self_attn.out_proj.weight\n",
      "model.encoder.layers.21.self_attn.out_proj.bias\n",
      "model.encoder.layers.21.self_attn_layer_norm.weight\n",
      "model.encoder.layers.21.self_attn_layer_norm.bias\n",
      "model.encoder.layers.21.fc1.weight\n",
      "model.encoder.layers.21.fc1.bias\n",
      "model.encoder.layers.21.fc2.weight\n",
      "model.encoder.layers.21.fc2.bias\n",
      "model.encoder.layers.21.final_layer_norm.weight\n",
      "model.encoder.layers.21.final_layer_norm.bias\n",
      "model.encoder.layers.22.self_attn.k_proj.weight\n",
      "model.encoder.layers.22.self_attn.v_proj.weight\n",
      "model.encoder.layers.22.self_attn.v_proj.bias\n",
      "model.encoder.layers.22.self_attn.q_proj.weight\n",
      "model.encoder.layers.22.self_attn.q_proj.bias\n",
      "model.encoder.layers.22.self_attn.out_proj.weight\n",
      "model.encoder.layers.22.self_attn.out_proj.bias\n",
      "model.encoder.layers.22.self_attn_layer_norm.weight\n",
      "model.encoder.layers.22.self_attn_layer_norm.bias\n",
      "model.encoder.layers.22.fc1.weight\n",
      "model.encoder.layers.22.fc1.bias\n",
      "model.encoder.layers.22.fc2.weight\n",
      "model.encoder.layers.22.fc2.bias\n",
      "model.encoder.layers.22.final_layer_norm.weight\n",
      "model.encoder.layers.22.final_layer_norm.bias\n",
      "model.encoder.layers.23.self_attn.k_proj.weight\n",
      "model.encoder.layers.23.self_attn.v_proj.weight\n",
      "model.encoder.layers.23.self_attn.v_proj.bias\n",
      "model.encoder.layers.23.self_attn.q_proj.weight\n",
      "model.encoder.layers.23.self_attn.q_proj.bias\n",
      "model.encoder.layers.23.self_attn.out_proj.weight\n",
      "model.encoder.layers.23.self_attn.out_proj.bias\n",
      "model.encoder.layers.23.self_attn_layer_norm.weight\n",
      "model.encoder.layers.23.self_attn_layer_norm.bias\n",
      "model.encoder.layers.23.fc1.weight\n",
      "model.encoder.layers.23.fc1.bias\n",
      "model.encoder.layers.23.fc2.weight\n",
      "model.encoder.layers.23.fc2.bias\n",
      "model.encoder.layers.23.final_layer_norm.weight\n",
      "model.encoder.layers.23.final_layer_norm.bias\n",
      "model.encoder.layer_norm.weight\n",
      "model.encoder.layer_norm.bias\n",
      "model.decoder.embed_tokens.weight\n",
      "model.decoder.embed_positions.weight\n",
      "model.decoder.layers.0.self_attn.k_proj.weight\n",
      "model.decoder.layers.0.self_attn.v_proj.weight\n",
      "model.decoder.layers.0.self_attn.v_proj.bias\n",
      "model.decoder.layers.0.self_attn.q_proj.weight\n",
      "model.decoder.layers.0.self_attn.q_proj.bias\n",
      "model.decoder.layers.0.self_attn.out_proj.weight\n",
      "model.decoder.layers.0.self_attn.out_proj.bias\n",
      "model.decoder.layers.0.self_attn_layer_norm.weight\n",
      "model.decoder.layers.0.self_attn_layer_norm.bias\n",
      "model.decoder.layers.0.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.0.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.0.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.0.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.0.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.0.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.0.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.0.fc1.weight\n",
      "model.decoder.layers.0.fc1.bias\n",
      "model.decoder.layers.0.fc2.weight\n",
      "model.decoder.layers.0.fc2.bias\n",
      "model.decoder.layers.0.final_layer_norm.weight\n",
      "model.decoder.layers.0.final_layer_norm.bias\n",
      "model.decoder.layers.1.self_attn.k_proj.weight\n",
      "model.decoder.layers.1.self_attn.v_proj.weight\n",
      "model.decoder.layers.1.self_attn.v_proj.bias\n",
      "model.decoder.layers.1.self_attn.q_proj.weight\n",
      "model.decoder.layers.1.self_attn.q_proj.bias\n",
      "model.decoder.layers.1.self_attn.out_proj.weight\n",
      "model.decoder.layers.1.self_attn.out_proj.bias\n",
      "model.decoder.layers.1.self_attn_layer_norm.weight\n",
      "model.decoder.layers.1.self_attn_layer_norm.bias\n",
      "model.decoder.layers.1.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.1.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.1.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.1.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.1.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.1.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.1.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.1.fc1.weight\n",
      "model.decoder.layers.1.fc1.bias\n",
      "model.decoder.layers.1.fc2.weight\n",
      "model.decoder.layers.1.fc2.bias\n",
      "model.decoder.layers.1.final_layer_norm.weight\n",
      "model.decoder.layers.1.final_layer_norm.bias\n",
      "model.decoder.layers.2.self_attn.k_proj.weight\n",
      "model.decoder.layers.2.self_attn.v_proj.weight\n",
      "model.decoder.layers.2.self_attn.v_proj.bias\n",
      "model.decoder.layers.2.self_attn.q_proj.weight\n",
      "model.decoder.layers.2.self_attn.q_proj.bias\n",
      "model.decoder.layers.2.self_attn.out_proj.weight\n",
      "model.decoder.layers.2.self_attn.out_proj.bias\n",
      "model.decoder.layers.2.self_attn_layer_norm.weight\n",
      "model.decoder.layers.2.self_attn_layer_norm.bias\n",
      "model.decoder.layers.2.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.2.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.2.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.2.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.2.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.2.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.2.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.2.fc1.weight\n",
      "model.decoder.layers.2.fc1.bias\n",
      "model.decoder.layers.2.fc2.weight\n",
      "model.decoder.layers.2.fc2.bias\n",
      "model.decoder.layers.2.final_layer_norm.weight\n",
      "model.decoder.layers.2.final_layer_norm.bias\n",
      "model.decoder.layers.3.self_attn.k_proj.weight\n",
      "model.decoder.layers.3.self_attn.v_proj.weight\n",
      "model.decoder.layers.3.self_attn.v_proj.bias\n",
      "model.decoder.layers.3.self_attn.q_proj.weight\n",
      "model.decoder.layers.3.self_attn.q_proj.bias\n",
      "model.decoder.layers.3.self_attn.out_proj.weight\n",
      "model.decoder.layers.3.self_attn.out_proj.bias\n",
      "model.decoder.layers.3.self_attn_layer_norm.weight\n",
      "model.decoder.layers.3.self_attn_layer_norm.bias\n",
      "model.decoder.layers.3.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.3.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.3.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.3.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.3.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.3.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.3.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.3.fc1.weight\n",
      "model.decoder.layers.3.fc1.bias\n",
      "model.decoder.layers.3.fc2.weight\n",
      "model.decoder.layers.3.fc2.bias\n",
      "model.decoder.layers.3.final_layer_norm.weight\n",
      "model.decoder.layers.3.final_layer_norm.bias\n",
      "model.decoder.layers.4.self_attn.k_proj.weight\n",
      "model.decoder.layers.4.self_attn.v_proj.weight\n",
      "model.decoder.layers.4.self_attn.v_proj.bias\n",
      "model.decoder.layers.4.self_attn.q_proj.weight\n",
      "model.decoder.layers.4.self_attn.q_proj.bias\n",
      "model.decoder.layers.4.self_attn.out_proj.weight\n",
      "model.decoder.layers.4.self_attn.out_proj.bias\n",
      "model.decoder.layers.4.self_attn_layer_norm.weight\n",
      "model.decoder.layers.4.self_attn_layer_norm.bias\n",
      "model.decoder.layers.4.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.4.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.4.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.4.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.4.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.4.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.4.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.4.fc1.weight\n",
      "model.decoder.layers.4.fc1.bias\n",
      "model.decoder.layers.4.fc2.weight\n",
      "model.decoder.layers.4.fc2.bias\n",
      "model.decoder.layers.4.final_layer_norm.weight\n",
      "model.decoder.layers.4.final_layer_norm.bias\n",
      "model.decoder.layers.5.self_attn.k_proj.weight\n",
      "model.decoder.layers.5.self_attn.v_proj.weight\n",
      "model.decoder.layers.5.self_attn.v_proj.bias\n",
      "model.decoder.layers.5.self_attn.q_proj.weight\n",
      "model.decoder.layers.5.self_attn.q_proj.bias\n",
      "model.decoder.layers.5.self_attn.out_proj.weight\n",
      "model.decoder.layers.5.self_attn.out_proj.bias\n",
      "model.decoder.layers.5.self_attn_layer_norm.weight\n",
      "model.decoder.layers.5.self_attn_layer_norm.bias\n",
      "model.decoder.layers.5.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.5.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.5.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.5.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.5.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.5.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.5.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.5.fc1.weight\n",
      "model.decoder.layers.5.fc1.bias\n",
      "model.decoder.layers.5.fc2.weight\n",
      "model.decoder.layers.5.fc2.bias\n",
      "model.decoder.layers.5.final_layer_norm.weight\n",
      "model.decoder.layers.5.final_layer_norm.bias\n",
      "model.decoder.layers.6.self_attn.k_proj.weight\n",
      "model.decoder.layers.6.self_attn.v_proj.weight\n",
      "model.decoder.layers.6.self_attn.v_proj.bias\n",
      "model.decoder.layers.6.self_attn.q_proj.weight\n",
      "model.decoder.layers.6.self_attn.q_proj.bias\n",
      "model.decoder.layers.6.self_attn.out_proj.weight\n",
      "model.decoder.layers.6.self_attn.out_proj.bias\n",
      "model.decoder.layers.6.self_attn_layer_norm.weight\n",
      "model.decoder.layers.6.self_attn_layer_norm.bias\n",
      "model.decoder.layers.6.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.6.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.6.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.6.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.6.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.6.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.6.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.6.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.6.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.6.fc1.weight\n",
      "model.decoder.layers.6.fc1.bias\n",
      "model.decoder.layers.6.fc2.weight\n",
      "model.decoder.layers.6.fc2.bias\n",
      "model.decoder.layers.6.final_layer_norm.weight\n",
      "model.decoder.layers.6.final_layer_norm.bias\n",
      "model.decoder.layers.7.self_attn.k_proj.weight\n",
      "model.decoder.layers.7.self_attn.v_proj.weight\n",
      "model.decoder.layers.7.self_attn.v_proj.bias\n",
      "model.decoder.layers.7.self_attn.q_proj.weight\n",
      "model.decoder.layers.7.self_attn.q_proj.bias\n",
      "model.decoder.layers.7.self_attn.out_proj.weight\n",
      "model.decoder.layers.7.self_attn.out_proj.bias\n",
      "model.decoder.layers.7.self_attn_layer_norm.weight\n",
      "model.decoder.layers.7.self_attn_layer_norm.bias\n",
      "model.decoder.layers.7.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.7.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.7.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.7.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.7.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.7.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.7.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.7.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.7.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.7.fc1.weight\n",
      "model.decoder.layers.7.fc1.bias\n",
      "model.decoder.layers.7.fc2.weight\n",
      "model.decoder.layers.7.fc2.bias\n",
      "model.decoder.layers.7.final_layer_norm.weight\n",
      "model.decoder.layers.7.final_layer_norm.bias\n",
      "model.decoder.layers.8.self_attn.k_proj.weight\n",
      "model.decoder.layers.8.self_attn.v_proj.weight\n",
      "model.decoder.layers.8.self_attn.v_proj.bias\n",
      "model.decoder.layers.8.self_attn.q_proj.weight\n",
      "model.decoder.layers.8.self_attn.q_proj.bias\n",
      "model.decoder.layers.8.self_attn.out_proj.weight\n",
      "model.decoder.layers.8.self_attn.out_proj.bias\n",
      "model.decoder.layers.8.self_attn_layer_norm.weight\n",
      "model.decoder.layers.8.self_attn_layer_norm.bias\n",
      "model.decoder.layers.8.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.8.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.8.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.8.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.8.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.8.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.8.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.8.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.8.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.8.fc1.weight\n",
      "model.decoder.layers.8.fc1.bias\n",
      "model.decoder.layers.8.fc2.weight\n",
      "model.decoder.layers.8.fc2.bias\n",
      "model.decoder.layers.8.final_layer_norm.weight\n",
      "model.decoder.layers.8.final_layer_norm.bias\n",
      "model.decoder.layers.9.self_attn.k_proj.weight\n",
      "model.decoder.layers.9.self_attn.v_proj.weight\n",
      "model.decoder.layers.9.self_attn.v_proj.bias\n",
      "model.decoder.layers.9.self_attn.q_proj.weight\n",
      "model.decoder.layers.9.self_attn.q_proj.bias\n",
      "model.decoder.layers.9.self_attn.out_proj.weight\n",
      "model.decoder.layers.9.self_attn.out_proj.bias\n",
      "model.decoder.layers.9.self_attn_layer_norm.weight\n",
      "model.decoder.layers.9.self_attn_layer_norm.bias\n",
      "model.decoder.layers.9.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.9.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.9.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.9.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.9.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.9.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.9.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.9.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.9.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.9.fc1.weight\n",
      "model.decoder.layers.9.fc1.bias\n",
      "model.decoder.layers.9.fc2.weight\n",
      "model.decoder.layers.9.fc2.bias\n",
      "model.decoder.layers.9.final_layer_norm.weight\n",
      "model.decoder.layers.9.final_layer_norm.bias\n",
      "model.decoder.layers.10.self_attn.k_proj.weight\n",
      "model.decoder.layers.10.self_attn.v_proj.weight\n",
      "model.decoder.layers.10.self_attn.v_proj.bias\n",
      "model.decoder.layers.10.self_attn.q_proj.weight\n",
      "model.decoder.layers.10.self_attn.q_proj.bias\n",
      "model.decoder.layers.10.self_attn.out_proj.weight\n",
      "model.decoder.layers.10.self_attn.out_proj.bias\n",
      "model.decoder.layers.10.self_attn_layer_norm.weight\n",
      "model.decoder.layers.10.self_attn_layer_norm.bias\n",
      "model.decoder.layers.10.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.10.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.10.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.10.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.10.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.10.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.10.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.10.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.10.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.10.fc1.weight\n",
      "model.decoder.layers.10.fc1.bias\n",
      "model.decoder.layers.10.fc2.weight\n",
      "model.decoder.layers.10.fc2.bias\n",
      "model.decoder.layers.10.final_layer_norm.weight\n",
      "model.decoder.layers.10.final_layer_norm.bias\n",
      "model.decoder.layers.11.self_attn.k_proj.weight\n",
      "model.decoder.layers.11.self_attn.v_proj.weight\n",
      "model.decoder.layers.11.self_attn.v_proj.bias\n",
      "model.decoder.layers.11.self_attn.q_proj.weight\n",
      "model.decoder.layers.11.self_attn.q_proj.bias\n",
      "model.decoder.layers.11.self_attn.out_proj.weight\n",
      "model.decoder.layers.11.self_attn.out_proj.bias\n",
      "model.decoder.layers.11.self_attn_layer_norm.weight\n",
      "model.decoder.layers.11.self_attn_layer_norm.bias\n",
      "model.decoder.layers.11.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.11.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.11.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.11.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.11.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.11.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.11.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.11.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.11.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.11.fc1.weight\n",
      "model.decoder.layers.11.fc1.bias\n",
      "model.decoder.layers.11.fc2.weight\n",
      "model.decoder.layers.11.fc2.bias\n",
      "model.decoder.layers.11.final_layer_norm.weight\n",
      "model.decoder.layers.11.final_layer_norm.bias\n",
      "model.decoder.layers.12.self_attn.k_proj.weight\n",
      "model.decoder.layers.12.self_attn.v_proj.weight\n",
      "model.decoder.layers.12.self_attn.v_proj.bias\n",
      "model.decoder.layers.12.self_attn.q_proj.weight\n",
      "model.decoder.layers.12.self_attn.q_proj.bias\n",
      "model.decoder.layers.12.self_attn.out_proj.weight\n",
      "model.decoder.layers.12.self_attn.out_proj.bias\n",
      "model.decoder.layers.12.self_attn_layer_norm.weight\n",
      "model.decoder.layers.12.self_attn_layer_norm.bias\n",
      "model.decoder.layers.12.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.12.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.12.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.12.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.12.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.12.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.12.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.12.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.12.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.12.fc1.weight\n",
      "model.decoder.layers.12.fc1.bias\n",
      "model.decoder.layers.12.fc2.weight\n",
      "model.decoder.layers.12.fc2.bias\n",
      "model.decoder.layers.12.final_layer_norm.weight\n",
      "model.decoder.layers.12.final_layer_norm.bias\n",
      "model.decoder.layers.13.self_attn.k_proj.weight\n",
      "model.decoder.layers.13.self_attn.v_proj.weight\n",
      "model.decoder.layers.13.self_attn.v_proj.bias\n",
      "model.decoder.layers.13.self_attn.q_proj.weight\n",
      "model.decoder.layers.13.self_attn.q_proj.bias\n",
      "model.decoder.layers.13.self_attn.out_proj.weight\n",
      "model.decoder.layers.13.self_attn.out_proj.bias\n",
      "model.decoder.layers.13.self_attn_layer_norm.weight\n",
      "model.decoder.layers.13.self_attn_layer_norm.bias\n",
      "model.decoder.layers.13.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.13.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.13.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.13.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.13.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.13.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.13.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.13.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.13.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.13.fc1.weight\n",
      "model.decoder.layers.13.fc1.bias\n",
      "model.decoder.layers.13.fc2.weight\n",
      "model.decoder.layers.13.fc2.bias\n",
      "model.decoder.layers.13.final_layer_norm.weight\n",
      "model.decoder.layers.13.final_layer_norm.bias\n",
      "model.decoder.layers.14.self_attn.k_proj.weight\n",
      "model.decoder.layers.14.self_attn.v_proj.weight\n",
      "model.decoder.layers.14.self_attn.v_proj.bias\n",
      "model.decoder.layers.14.self_attn.q_proj.weight\n",
      "model.decoder.layers.14.self_attn.q_proj.bias\n",
      "model.decoder.layers.14.self_attn.out_proj.weight\n",
      "model.decoder.layers.14.self_attn.out_proj.bias\n",
      "model.decoder.layers.14.self_attn_layer_norm.weight\n",
      "model.decoder.layers.14.self_attn_layer_norm.bias\n",
      "model.decoder.layers.14.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.14.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.14.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.14.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.14.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.14.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.14.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.14.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.14.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.14.fc1.weight\n",
      "model.decoder.layers.14.fc1.bias\n",
      "model.decoder.layers.14.fc2.weight\n",
      "model.decoder.layers.14.fc2.bias\n",
      "model.decoder.layers.14.final_layer_norm.weight\n",
      "model.decoder.layers.14.final_layer_norm.bias\n",
      "model.decoder.layers.15.self_attn.k_proj.weight\n",
      "model.decoder.layers.15.self_attn.v_proj.weight\n",
      "model.decoder.layers.15.self_attn.v_proj.bias\n",
      "model.decoder.layers.15.self_attn.q_proj.weight\n",
      "model.decoder.layers.15.self_attn.q_proj.bias\n",
      "model.decoder.layers.15.self_attn.out_proj.weight\n",
      "model.decoder.layers.15.self_attn.out_proj.bias\n",
      "model.decoder.layers.15.self_attn_layer_norm.weight\n",
      "model.decoder.layers.15.self_attn_layer_norm.bias\n",
      "model.decoder.layers.15.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.15.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.15.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.15.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.15.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.15.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.15.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.15.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.15.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.15.fc1.weight\n",
      "model.decoder.layers.15.fc1.bias\n",
      "model.decoder.layers.15.fc2.weight\n",
      "model.decoder.layers.15.fc2.bias\n",
      "model.decoder.layers.15.final_layer_norm.weight\n",
      "model.decoder.layers.15.final_layer_norm.bias\n",
      "model.decoder.layers.16.self_attn.k_proj.weight\n",
      "model.decoder.layers.16.self_attn.v_proj.weight\n",
      "model.decoder.layers.16.self_attn.v_proj.bias\n",
      "model.decoder.layers.16.self_attn.q_proj.weight\n",
      "model.decoder.layers.16.self_attn.q_proj.bias\n",
      "model.decoder.layers.16.self_attn.out_proj.weight\n",
      "model.decoder.layers.16.self_attn.out_proj.bias\n",
      "model.decoder.layers.16.self_attn_layer_norm.weight\n",
      "model.decoder.layers.16.self_attn_layer_norm.bias\n",
      "model.decoder.layers.16.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.16.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.16.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.16.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.16.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.16.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.16.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.16.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.16.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.16.fc1.weight\n",
      "model.decoder.layers.16.fc1.bias\n",
      "model.decoder.layers.16.fc2.weight\n",
      "model.decoder.layers.16.fc2.bias\n",
      "model.decoder.layers.16.final_layer_norm.weight\n",
      "model.decoder.layers.16.final_layer_norm.bias\n",
      "model.decoder.layers.17.self_attn.k_proj.weight\n",
      "model.decoder.layers.17.self_attn.v_proj.weight\n",
      "model.decoder.layers.17.self_attn.v_proj.bias\n",
      "model.decoder.layers.17.self_attn.q_proj.weight\n",
      "model.decoder.layers.17.self_attn.q_proj.bias\n",
      "model.decoder.layers.17.self_attn.out_proj.weight\n",
      "model.decoder.layers.17.self_attn.out_proj.bias\n",
      "model.decoder.layers.17.self_attn_layer_norm.weight\n",
      "model.decoder.layers.17.self_attn_layer_norm.bias\n",
      "model.decoder.layers.17.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.17.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.17.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.17.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.17.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.17.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.17.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.17.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.17.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.17.fc1.weight\n",
      "model.decoder.layers.17.fc1.bias\n",
      "model.decoder.layers.17.fc2.weight\n",
      "model.decoder.layers.17.fc2.bias\n",
      "model.decoder.layers.17.final_layer_norm.weight\n",
      "model.decoder.layers.17.final_layer_norm.bias\n",
      "model.decoder.layers.18.self_attn.k_proj.weight\n",
      "model.decoder.layers.18.self_attn.v_proj.weight\n",
      "model.decoder.layers.18.self_attn.v_proj.bias\n",
      "model.decoder.layers.18.self_attn.q_proj.weight\n",
      "model.decoder.layers.18.self_attn.q_proj.bias\n",
      "model.decoder.layers.18.self_attn.out_proj.weight\n",
      "model.decoder.layers.18.self_attn.out_proj.bias\n",
      "model.decoder.layers.18.self_attn_layer_norm.weight\n",
      "model.decoder.layers.18.self_attn_layer_norm.bias\n",
      "model.decoder.layers.18.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.18.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.18.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.18.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.18.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.18.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.18.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.18.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.18.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.18.fc1.weight\n",
      "model.decoder.layers.18.fc1.bias\n",
      "model.decoder.layers.18.fc2.weight\n",
      "model.decoder.layers.18.fc2.bias\n",
      "model.decoder.layers.18.final_layer_norm.weight\n",
      "model.decoder.layers.18.final_layer_norm.bias\n",
      "model.decoder.layers.19.self_attn.k_proj.weight\n",
      "model.decoder.layers.19.self_attn.v_proj.weight\n",
      "model.decoder.layers.19.self_attn.v_proj.bias\n",
      "model.decoder.layers.19.self_attn.q_proj.weight\n",
      "model.decoder.layers.19.self_attn.q_proj.bias\n",
      "model.decoder.layers.19.self_attn.out_proj.weight\n",
      "model.decoder.layers.19.self_attn.out_proj.bias\n",
      "model.decoder.layers.19.self_attn_layer_norm.weight\n",
      "model.decoder.layers.19.self_attn_layer_norm.bias\n",
      "model.decoder.layers.19.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.19.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.19.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.19.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.19.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.19.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.19.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.19.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.19.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.19.fc1.weight\n",
      "model.decoder.layers.19.fc1.bias\n",
      "model.decoder.layers.19.fc2.weight\n",
      "model.decoder.layers.19.fc2.bias\n",
      "model.decoder.layers.19.final_layer_norm.weight\n",
      "model.decoder.layers.19.final_layer_norm.bias\n",
      "model.decoder.layers.20.self_attn.k_proj.weight\n",
      "model.decoder.layers.20.self_attn.v_proj.weight\n",
      "model.decoder.layers.20.self_attn.v_proj.bias\n",
      "model.decoder.layers.20.self_attn.q_proj.weight\n",
      "model.decoder.layers.20.self_attn.q_proj.bias\n",
      "model.decoder.layers.20.self_attn.out_proj.weight\n",
      "model.decoder.layers.20.self_attn.out_proj.bias\n",
      "model.decoder.layers.20.self_attn_layer_norm.weight\n",
      "model.decoder.layers.20.self_attn_layer_norm.bias\n",
      "model.decoder.layers.20.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.20.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.20.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.20.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.20.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.20.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.20.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.20.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.20.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.20.fc1.weight\n",
      "model.decoder.layers.20.fc1.bias\n",
      "model.decoder.layers.20.fc2.weight\n",
      "model.decoder.layers.20.fc2.bias\n",
      "model.decoder.layers.20.final_layer_norm.weight\n",
      "model.decoder.layers.20.final_layer_norm.bias\n",
      "model.decoder.layers.21.self_attn.k_proj.weight\n",
      "model.decoder.layers.21.self_attn.v_proj.weight\n",
      "model.decoder.layers.21.self_attn.v_proj.bias\n",
      "model.decoder.layers.21.self_attn.q_proj.weight\n",
      "model.decoder.layers.21.self_attn.q_proj.bias\n",
      "model.decoder.layers.21.self_attn.out_proj.weight\n",
      "model.decoder.layers.21.self_attn.out_proj.bias\n",
      "model.decoder.layers.21.self_attn_layer_norm.weight\n",
      "model.decoder.layers.21.self_attn_layer_norm.bias\n",
      "model.decoder.layers.21.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.21.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.21.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.21.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.21.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.21.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.21.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.21.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.21.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.21.fc1.weight\n",
      "model.decoder.layers.21.fc1.bias\n",
      "model.decoder.layers.21.fc2.weight\n",
      "model.decoder.layers.21.fc2.bias\n",
      "model.decoder.layers.21.final_layer_norm.weight\n",
      "model.decoder.layers.21.final_layer_norm.bias\n",
      "model.decoder.layers.22.self_attn.k_proj.weight\n",
      "model.decoder.layers.22.self_attn.v_proj.weight\n",
      "model.decoder.layers.22.self_attn.v_proj.bias\n",
      "model.decoder.layers.22.self_attn.q_proj.weight\n",
      "model.decoder.layers.22.self_attn.q_proj.bias\n",
      "model.decoder.layers.22.self_attn.out_proj.weight\n",
      "model.decoder.layers.22.self_attn.out_proj.bias\n",
      "model.decoder.layers.22.self_attn_layer_norm.weight\n",
      "model.decoder.layers.22.self_attn_layer_norm.bias\n",
      "model.decoder.layers.22.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.22.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.22.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.22.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.22.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.22.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.22.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.22.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.22.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.22.fc1.weight\n",
      "model.decoder.layers.22.fc1.bias\n",
      "model.decoder.layers.22.fc2.weight\n",
      "model.decoder.layers.22.fc2.bias\n",
      "model.decoder.layers.22.final_layer_norm.weight\n",
      "model.decoder.layers.22.final_layer_norm.bias\n",
      "model.decoder.layers.23.self_attn.k_proj.weight\n",
      "model.decoder.layers.23.self_attn.v_proj.weight\n",
      "model.decoder.layers.23.self_attn.v_proj.bias\n",
      "model.decoder.layers.23.self_attn.q_proj.weight\n",
      "model.decoder.layers.23.self_attn.q_proj.bias\n",
      "model.decoder.layers.23.self_attn.out_proj.weight\n",
      "model.decoder.layers.23.self_attn.out_proj.bias\n",
      "model.decoder.layers.23.self_attn_layer_norm.weight\n",
      "model.decoder.layers.23.self_attn_layer_norm.bias\n",
      "model.decoder.layers.23.encoder_attn.k_proj.weight\n",
      "model.decoder.layers.23.encoder_attn.v_proj.weight\n",
      "model.decoder.layers.23.encoder_attn.v_proj.bias\n",
      "model.decoder.layers.23.encoder_attn.q_proj.weight\n",
      "model.decoder.layers.23.encoder_attn.q_proj.bias\n",
      "model.decoder.layers.23.encoder_attn.out_proj.weight\n",
      "model.decoder.layers.23.encoder_attn.out_proj.bias\n",
      "model.decoder.layers.23.encoder_attn_layer_norm.weight\n",
      "model.decoder.layers.23.encoder_attn_layer_norm.bias\n",
      "model.decoder.layers.23.fc1.weight\n",
      "model.decoder.layers.23.fc1.bias\n",
      "model.decoder.layers.23.fc2.weight\n",
      "model.decoder.layers.23.fc2.bias\n",
      "model.decoder.layers.23.final_layer_norm.weight\n",
      "model.decoder.layers.23.final_layer_norm.bias\n",
      "model.decoder.layer_norm.weight\n",
      "model.decoder.layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "for name, params in model_hf.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('encoder.conv1.weight',\n",
       " Parameter containing:\n",
       " tensor([[[-0.0040, -0.0121, -0.0248],\n",
       "          [ 0.0225,  0.0288,  0.0167],\n",
       "          [ 0.0295,  0.0192, -0.0166],\n",
       "          ...,\n",
       "          [-0.0424, -0.0015,  0.0562],\n",
       "          [-0.0447, -0.0056,  0.0519],\n",
       "          [-0.0721, -0.0318,  0.0418]],\n",
       " \n",
       "         [[-0.0002, -0.0023, -0.0048],\n",
       "          [-0.0082, -0.0153, -0.0153],\n",
       "          [ 0.0084,  0.0279,  0.0320],\n",
       "          ...,\n",
       "          [-0.0065, -0.0031, -0.0032],\n",
       "          [-0.0088,  0.0019,  0.0014],\n",
       "          [-0.0134, -0.0021, -0.0022]],\n",
       " \n",
       "         [[ 0.0083,  0.0136,  0.0189],\n",
       "          [-0.0099, -0.0051, -0.0089],\n",
       "          [ 0.0188,  0.0259,  0.0311],\n",
       "          ...,\n",
       "          [ 0.0088, -0.0045, -0.0211],\n",
       "          [ 0.0067, -0.0074, -0.0190],\n",
       "          [-0.0006, -0.0187, -0.0304]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0019,  0.0037,  0.0033],\n",
       "          [ 0.0047,  0.0044, -0.0022],\n",
       "          [ 0.0015,  0.0018, -0.0069],\n",
       "          ...,\n",
       "          [-0.0493,  0.0188, -0.0026],\n",
       "          [-0.0432,  0.0179, -0.0093],\n",
       "          [-0.0108,  0.0762,  0.0426]],\n",
       " \n",
       "         [[-0.0072,  0.0019,  0.0049],\n",
       "          [ 0.0042,  0.0238,  0.0223],\n",
       "          [-0.0129, -0.0018, -0.0004],\n",
       "          ...,\n",
       "          [ 0.0002, -0.0018, -0.0037],\n",
       "          [ 0.0035,  0.0087,  0.0001],\n",
       "          [ 0.0198,  0.0337,  0.0175]],\n",
       " \n",
       "         [[-0.0028, -0.0086, -0.0043],\n",
       "          [-0.0129, -0.0195, -0.0082],\n",
       "          [-0.0351, -0.0293,  0.0058],\n",
       "          ...,\n",
       "          [ 0.0012, -0.0012, -0.0146],\n",
       "          [ 0.0017, -0.0012, -0.0098],\n",
       "          [-0.0007,  0.0035, -0.0161]]], requires_grad=True))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model.encoder.conv1.weight',\n",
       " Parameter containing:\n",
       " tensor([[[-0.0040, -0.0121, -0.0248],\n",
       "          [ 0.0225,  0.0288,  0.0167],\n",
       "          [ 0.0295,  0.0192, -0.0166],\n",
       "          ...,\n",
       "          [-0.0424, -0.0015,  0.0562],\n",
       "          [-0.0447, -0.0056,  0.0519],\n",
       "          [-0.0721, -0.0318,  0.0418]],\n",
       " \n",
       "         [[-0.0002, -0.0023, -0.0048],\n",
       "          [-0.0082, -0.0153, -0.0153],\n",
       "          [ 0.0084,  0.0279,  0.0320],\n",
       "          ...,\n",
       "          [-0.0065, -0.0031, -0.0032],\n",
       "          [-0.0088,  0.0019,  0.0014],\n",
       "          [-0.0134, -0.0021, -0.0022]],\n",
       " \n",
       "         [[ 0.0083,  0.0136,  0.0189],\n",
       "          [-0.0099, -0.0051, -0.0089],\n",
       "          [ 0.0188,  0.0259,  0.0311],\n",
       "          ...,\n",
       "          [ 0.0088, -0.0045, -0.0211],\n",
       "          [ 0.0067, -0.0074, -0.0190],\n",
       "          [-0.0006, -0.0187, -0.0304]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.0019,  0.0037,  0.0033],\n",
       "          [ 0.0047,  0.0044, -0.0022],\n",
       "          [ 0.0015,  0.0018, -0.0069],\n",
       "          ...,\n",
       "          [-0.0493,  0.0188, -0.0026],\n",
       "          [-0.0432,  0.0179, -0.0093],\n",
       "          [-0.0108,  0.0762,  0.0426]],\n",
       " \n",
       "         [[-0.0072,  0.0019,  0.0049],\n",
       "          [ 0.0042,  0.0238,  0.0223],\n",
       "          [-0.0129, -0.0018, -0.0004],\n",
       "          ...,\n",
       "          [ 0.0002, -0.0018, -0.0037],\n",
       "          [ 0.0035,  0.0087,  0.0001],\n",
       "          [ 0.0198,  0.0337,  0.0175]],\n",
       " \n",
       "         [[-0.0028, -0.0086, -0.0043],\n",
       "          [-0.0129, -0.0195, -0.0082],\n",
       "          [-0.0351, -0.0293,  0.0058],\n",
       "          ...,\n",
       "          [ 0.0012, -0.0012, -0.0146],\n",
       "          [ 0.0017, -0.0012, -0.0098],\n",
       "          [-0.0007,  0.0035, -0.0161]]], requires_grad=True))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model_hf.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-chain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
